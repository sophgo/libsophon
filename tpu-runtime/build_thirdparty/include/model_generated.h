// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_MODEL_BMODEL_H_
#define FLATBUFFERS_GENERATED_MODEL_BMODEL_H_

#include "flatbuffers/flatbuffers.h"

namespace bmodel {

struct Binary;

struct Shape;
struct ShapeT;

struct CmdGroup;
struct CmdGroupT;

struct CoreCommands;
struct CoreCommandsT;

struct StageIR;
struct StageIRT;

struct Location;
struct LocationT;

struct CoeffMem;
struct CoeffMemT;

struct Tensor;
struct TensorT;

struct CpuConst;
struct CpuConstT;

struct CpuParam;
struct CpuParamT;

struct OutputFrom;
struct OutputFromT;

struct MergeParam;
struct MergeParamT;

struct SwitchParam;
struct SwitchParamT;

struct SubNet;
struct SubNetT;

struct NetStatic;
struct NetStaticT;

struct NetDynamic;
struct NetDynamicT;

struct NetParameter;
struct NetParameterT;

struct Cascade;
struct CascadeT;

struct Net;
struct NetT;

struct KernelModule;
struct KernelModuleT;

struct CpuopModule;
struct CpuopModuleT;

struct Model;
struct ModelT;

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(8) Binary FLATBUFFERS_FINAL_CLASS {
 private:
  uint64_t start_;
  uint64_t size_;

 public:
  Binary() {
    memset(this, 0, sizeof(Binary));
  }
  Binary(uint64_t _start, uint64_t _size)
      : start_(flatbuffers::EndianScalar(_start)),
        size_(flatbuffers::EndianScalar(_size)) {
  }
  uint64_t start() const {
    return flatbuffers::EndianScalar(start_);
  }
  void mutate_start(uint64_t _start) {
    flatbuffers::WriteScalar(&start_, _start);
  }
  uint64_t size() const {
    return flatbuffers::EndianScalar(size_);
  }
  void mutate_size(uint64_t _size) {
    flatbuffers::WriteScalar(&size_, _size);
  }
};
FLATBUFFERS_STRUCT_END(Binary, 16);

struct ShapeT : public flatbuffers::NativeTable {
  typedef Shape TableType;
  std::vector<uint64_t> dim;
  ShapeT() {
  }
};

struct Shape FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef ShapeT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  const flatbuffers::Vector<uint64_t> *dim() const {
    return GetPointer<const flatbuffers::Vector<uint64_t> *>(VT_DIM);
  }
  flatbuffers::Vector<uint64_t> *mutable_dim() {
    return GetPointer<flatbuffers::Vector<uint64_t> *>(VT_DIM);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIM) &&
           verifier.VerifyVector(dim()) &&
           verifier.EndTable();
  }
  ShapeT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ShapeT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Shape> Pack(flatbuffers::FlatBufferBuilder &_fbb, const ShapeT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ShapeBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_dim(flatbuffers::Offset<flatbuffers::Vector<uint64_t>> dim) {
    fbb_.AddOffset(Shape::VT_DIM, dim);
  }
  explicit ShapeBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ShapeBuilder &operator=(const ShapeBuilder &);
  flatbuffers::Offset<Shape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Shape>(end);
    return o;
  }
};

inline flatbuffers::Offset<Shape> CreateShape(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<uint64_t>> dim = 0) {
  ShapeBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

inline flatbuffers::Offset<Shape> CreateShapeDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint64_t> *dim = nullptr) {
  auto dim__ = dim ? _fbb.CreateVector<uint64_t>(*dim) : 0;
  return bmodel::CreateShape(
      _fbb,
      dim__);
}

flatbuffers::Offset<Shape> CreateShape(flatbuffers::FlatBufferBuilder &_fbb, const ShapeT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CmdGroupT : public flatbuffers::NativeTable {
  typedef CmdGroup TableType;
  uint32_t bdc_num;
  uint32_t gdma_num;
  std::unique_ptr<Binary> binary_bdc;
  std::unique_ptr<Binary> binary_gdma;
  uint32_t bdc_cmd_byte;
  uint32_t gdma_cmd_byte;
  CmdGroupT()
      : bdc_num(0),
        gdma_num(0),
        bdc_cmd_byte(0),
        gdma_cmd_byte(0) {
  }
};

struct CmdGroup FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CmdGroupT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BDC_NUM = 4,
    VT_GDMA_NUM = 6,
    VT_BINARY_BDC = 8,
    VT_BINARY_GDMA = 10,
    VT_BDC_CMD_BYTE = 12,
    VT_GDMA_CMD_BYTE = 14
  };
  uint32_t bdc_num() const {
    return GetField<uint32_t>(VT_BDC_NUM, 0);
  }
  bool mutate_bdc_num(uint32_t _bdc_num) {
    return SetField<uint32_t>(VT_BDC_NUM, _bdc_num, 0);
  }
  uint32_t gdma_num() const {
    return GetField<uint32_t>(VT_GDMA_NUM, 0);
  }
  bool mutate_gdma_num(uint32_t _gdma_num) {
    return SetField<uint32_t>(VT_GDMA_NUM, _gdma_num, 0);
  }
  const Binary *binary_bdc() const {
    return GetStruct<const Binary *>(VT_BINARY_BDC);
  }
  Binary *mutable_binary_bdc() {
    return GetStruct<Binary *>(VT_BINARY_BDC);
  }
  const Binary *binary_gdma() const {
    return GetStruct<const Binary *>(VT_BINARY_GDMA);
  }
  Binary *mutable_binary_gdma() {
    return GetStruct<Binary *>(VT_BINARY_GDMA);
  }
  uint32_t bdc_cmd_byte() const {
    return GetField<uint32_t>(VT_BDC_CMD_BYTE, 0);
  }
  bool mutate_bdc_cmd_byte(uint32_t _bdc_cmd_byte) {
    return SetField<uint32_t>(VT_BDC_CMD_BYTE, _bdc_cmd_byte, 0);
  }
  uint32_t gdma_cmd_byte() const {
    return GetField<uint32_t>(VT_GDMA_CMD_BYTE, 0);
  }
  bool mutate_gdma_cmd_byte(uint32_t _gdma_cmd_byte) {
    return SetField<uint32_t>(VT_GDMA_CMD_BYTE, _gdma_cmd_byte, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_BDC_NUM) &&
           VerifyField<uint32_t>(verifier, VT_GDMA_NUM) &&
           VerifyField<Binary>(verifier, VT_BINARY_BDC) &&
           VerifyField<Binary>(verifier, VT_BINARY_GDMA) &&
           VerifyField<uint32_t>(verifier, VT_BDC_CMD_BYTE) &&
           VerifyField<uint32_t>(verifier, VT_GDMA_CMD_BYTE) &&
           verifier.EndTable();
  }
  CmdGroupT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CmdGroupT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<CmdGroup> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CmdGroupT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CmdGroupBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_bdc_num(uint32_t bdc_num) {
    fbb_.AddElement<uint32_t>(CmdGroup::VT_BDC_NUM, bdc_num, 0);
  }
  void add_gdma_num(uint32_t gdma_num) {
    fbb_.AddElement<uint32_t>(CmdGroup::VT_GDMA_NUM, gdma_num, 0);
  }
  void add_binary_bdc(const Binary *binary_bdc) {
    fbb_.AddStruct(CmdGroup::VT_BINARY_BDC, binary_bdc);
  }
  void add_binary_gdma(const Binary *binary_gdma) {
    fbb_.AddStruct(CmdGroup::VT_BINARY_GDMA, binary_gdma);
  }
  void add_bdc_cmd_byte(uint32_t bdc_cmd_byte) {
    fbb_.AddElement<uint32_t>(CmdGroup::VT_BDC_CMD_BYTE, bdc_cmd_byte, 0);
  }
  void add_gdma_cmd_byte(uint32_t gdma_cmd_byte) {
    fbb_.AddElement<uint32_t>(CmdGroup::VT_GDMA_CMD_BYTE, gdma_cmd_byte, 0);
  }
  explicit CmdGroupBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CmdGroupBuilder &operator=(const CmdGroupBuilder &);
  flatbuffers::Offset<CmdGroup> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CmdGroup>(end);
    return o;
  }
};

inline flatbuffers::Offset<CmdGroup> CreateCmdGroup(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t bdc_num = 0,
    uint32_t gdma_num = 0,
    const Binary *binary_bdc = 0,
    const Binary *binary_gdma = 0,
    uint32_t bdc_cmd_byte = 0,
    uint32_t gdma_cmd_byte = 0) {
  CmdGroupBuilder builder_(_fbb);
  builder_.add_gdma_cmd_byte(gdma_cmd_byte);
  builder_.add_bdc_cmd_byte(bdc_cmd_byte);
  builder_.add_binary_gdma(binary_gdma);
  builder_.add_binary_bdc(binary_bdc);
  builder_.add_gdma_num(gdma_num);
  builder_.add_bdc_num(bdc_num);
  return builder_.Finish();
}

flatbuffers::Offset<CmdGroup> CreateCmdGroup(flatbuffers::FlatBufferBuilder &_fbb, const CmdGroupT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CoreCommandsT : public flatbuffers::NativeTable {
  typedef CoreCommands TableType;
  std::vector<std::unique_ptr<CmdGroupT>> gdma_tiu_commands;
  std::vector<Binary> sdma_commands;
  std::vector<Binary> hau_commands;
  std::vector<Binary> cdma_commands;
  CoreCommandsT() {
  }
};

struct CoreCommands FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CoreCommandsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_GDMA_TIU_COMMANDS = 4,
    VT_SDMA_COMMANDS = 6,
    VT_HAU_COMMANDS = 8,
    VT_CDMA_COMMANDS = 10
  };
  const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *gdma_tiu_commands() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_GDMA_TIU_COMMANDS);
  }
  flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *mutable_gdma_tiu_commands() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_GDMA_TIU_COMMANDS);
  }
  const flatbuffers::Vector<const Binary *> *sdma_commands() const {
    return GetPointer<const flatbuffers::Vector<const Binary *> *>(VT_SDMA_COMMANDS);
  }
  flatbuffers::Vector<const Binary *> *mutable_sdma_commands() {
    return GetPointer<flatbuffers::Vector<const Binary *> *>(VT_SDMA_COMMANDS);
  }
  const flatbuffers::Vector<const Binary *> *hau_commands() const {
    return GetPointer<const flatbuffers::Vector<const Binary *> *>(VT_HAU_COMMANDS);
  }
  flatbuffers::Vector<const Binary *> *mutable_hau_commands() {
    return GetPointer<flatbuffers::Vector<const Binary *> *>(VT_HAU_COMMANDS);
  }
  const flatbuffers::Vector<const Binary *> *cdma_commands() const {
    return GetPointer<const flatbuffers::Vector<const Binary *> *>(VT_CDMA_COMMANDS);
  }
  flatbuffers::Vector<const Binary *> *mutable_cdma_commands() {
    return GetPointer<flatbuffers::Vector<const Binary *> *>(VT_CDMA_COMMANDS);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_GDMA_TIU_COMMANDS) &&
           verifier.VerifyVector(gdma_tiu_commands()) &&
           verifier.VerifyVectorOfTables(gdma_tiu_commands()) &&
           VerifyOffset(verifier, VT_SDMA_COMMANDS) &&
           verifier.VerifyVector(sdma_commands()) &&
           VerifyOffset(verifier, VT_HAU_COMMANDS) &&
           verifier.VerifyVector(hau_commands()) &&
           VerifyOffset(verifier, VT_CDMA_COMMANDS) &&
           verifier.VerifyVector(cdma_commands()) &&
           verifier.EndTable();
  }
  CoreCommandsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CoreCommandsT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<CoreCommands> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CoreCommandsT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CoreCommandsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_gdma_tiu_commands(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> gdma_tiu_commands) {
    fbb_.AddOffset(CoreCommands::VT_GDMA_TIU_COMMANDS, gdma_tiu_commands);
  }
  void add_sdma_commands(flatbuffers::Offset<flatbuffers::Vector<const Binary *>> sdma_commands) {
    fbb_.AddOffset(CoreCommands::VT_SDMA_COMMANDS, sdma_commands);
  }
  void add_hau_commands(flatbuffers::Offset<flatbuffers::Vector<const Binary *>> hau_commands) {
    fbb_.AddOffset(CoreCommands::VT_HAU_COMMANDS, hau_commands);
  }
  void add_cdma_commands(flatbuffers::Offset<flatbuffers::Vector<const Binary *>> cdma_commands) {
    fbb_.AddOffset(CoreCommands::VT_CDMA_COMMANDS, cdma_commands);
  }
  explicit CoreCommandsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CoreCommandsBuilder &operator=(const CoreCommandsBuilder &);
  flatbuffers::Offset<CoreCommands> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CoreCommands>(end);
    return o;
  }
};

inline flatbuffers::Offset<CoreCommands> CreateCoreCommands(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> gdma_tiu_commands = 0,
    flatbuffers::Offset<flatbuffers::Vector<const Binary *>> sdma_commands = 0,
    flatbuffers::Offset<flatbuffers::Vector<const Binary *>> hau_commands = 0,
    flatbuffers::Offset<flatbuffers::Vector<const Binary *>> cdma_commands = 0) {
  CoreCommandsBuilder builder_(_fbb);
  builder_.add_cdma_commands(cdma_commands);
  builder_.add_hau_commands(hau_commands);
  builder_.add_sdma_commands(sdma_commands);
  builder_.add_gdma_tiu_commands(gdma_tiu_commands);
  return builder_.Finish();
}

inline flatbuffers::Offset<CoreCommands> CreateCoreCommandsDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<flatbuffers::Offset<CmdGroup>> *gdma_tiu_commands = nullptr,
    const std::vector<Binary> *sdma_commands = nullptr,
    const std::vector<Binary> *hau_commands = nullptr,
    const std::vector<Binary> *cdma_commands = nullptr) {
  auto gdma_tiu_commands__ = gdma_tiu_commands ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>>(*gdma_tiu_commands) : 0;
  auto sdma_commands__ = sdma_commands ? _fbb.CreateVectorOfStructs<Binary>(*sdma_commands) : 0;
  auto hau_commands__ = hau_commands ? _fbb.CreateVectorOfStructs<Binary>(*hau_commands) : 0;
  auto cdma_commands__ = cdma_commands ? _fbb.CreateVectorOfStructs<Binary>(*cdma_commands) : 0;
  return bmodel::CreateCoreCommands(
      _fbb,
      gdma_tiu_commands__,
      sdma_commands__,
      hau_commands__,
      cdma_commands__);
}

flatbuffers::Offset<CoreCommands> CreateCoreCommands(flatbuffers::FlatBufferBuilder &_fbb, const CoreCommandsT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct StageIRT : public flatbuffers::NativeTable {
  typedef StageIR TableType;
  uint32_t ir_info_len;
  int32_t height_high;
  int32_t height_low;
  int32_t width_high;
  int32_t width_low;
  StageIRT()
      : ir_info_len(0),
        height_high(0),
        height_low(0),
        width_high(0),
        width_low(0) {
  }
};

struct StageIR FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef StageIRT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IR_INFO_LEN = 4,
    VT_HEIGHT_HIGH = 6,
    VT_HEIGHT_LOW = 8,
    VT_WIDTH_HIGH = 10,
    VT_WIDTH_LOW = 12
  };
  uint32_t ir_info_len() const {
    return GetField<uint32_t>(VT_IR_INFO_LEN, 0);
  }
  bool mutate_ir_info_len(uint32_t _ir_info_len) {
    return SetField<uint32_t>(VT_IR_INFO_LEN, _ir_info_len, 0);
  }
  int32_t height_high() const {
    return GetField<int32_t>(VT_HEIGHT_HIGH, 0);
  }
  bool mutate_height_high(int32_t _height_high) {
    return SetField<int32_t>(VT_HEIGHT_HIGH, _height_high, 0);
  }
  int32_t height_low() const {
    return GetField<int32_t>(VT_HEIGHT_LOW, 0);
  }
  bool mutate_height_low(int32_t _height_low) {
    return SetField<int32_t>(VT_HEIGHT_LOW, _height_low, 0);
  }
  int32_t width_high() const {
    return GetField<int32_t>(VT_WIDTH_HIGH, 0);
  }
  bool mutate_width_high(int32_t _width_high) {
    return SetField<int32_t>(VT_WIDTH_HIGH, _width_high, 0);
  }
  int32_t width_low() const {
    return GetField<int32_t>(VT_WIDTH_LOW, 0);
  }
  bool mutate_width_low(int32_t _width_low) {
    return SetField<int32_t>(VT_WIDTH_LOW, _width_low, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_IR_INFO_LEN) &&
           VerifyField<int32_t>(verifier, VT_HEIGHT_HIGH) &&
           VerifyField<int32_t>(verifier, VT_HEIGHT_LOW) &&
           VerifyField<int32_t>(verifier, VT_WIDTH_HIGH) &&
           VerifyField<int32_t>(verifier, VT_WIDTH_LOW) &&
           verifier.EndTable();
  }
  StageIRT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(StageIRT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<StageIR> Pack(flatbuffers::FlatBufferBuilder &_fbb, const StageIRT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct StageIRBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_ir_info_len(uint32_t ir_info_len) {
    fbb_.AddElement<uint32_t>(StageIR::VT_IR_INFO_LEN, ir_info_len, 0);
  }
  void add_height_high(int32_t height_high) {
    fbb_.AddElement<int32_t>(StageIR::VT_HEIGHT_HIGH, height_high, 0);
  }
  void add_height_low(int32_t height_low) {
    fbb_.AddElement<int32_t>(StageIR::VT_HEIGHT_LOW, height_low, 0);
  }
  void add_width_high(int32_t width_high) {
    fbb_.AddElement<int32_t>(StageIR::VT_WIDTH_HIGH, width_high, 0);
  }
  void add_width_low(int32_t width_low) {
    fbb_.AddElement<int32_t>(StageIR::VT_WIDTH_LOW, width_low, 0);
  }
  explicit StageIRBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  StageIRBuilder &operator=(const StageIRBuilder &);
  flatbuffers::Offset<StageIR> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<StageIR>(end);
    return o;
  }
};

inline flatbuffers::Offset<StageIR> CreateStageIR(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t ir_info_len = 0,
    int32_t height_high = 0,
    int32_t height_low = 0,
    int32_t width_high = 0,
    int32_t width_low = 0) {
  StageIRBuilder builder_(_fbb);
  builder_.add_width_low(width_low);
  builder_.add_width_high(width_high);
  builder_.add_height_low(height_low);
  builder_.add_height_high(height_high);
  builder_.add_ir_info_len(ir_info_len);
  return builder_.Finish();
}

flatbuffers::Offset<StageIR> CreateStageIR(flatbuffers::FlatBufferBuilder &_fbb, const StageIRT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct LocationT : public flatbuffers::NativeTable {
  typedef Location TableType;
  std::string name;
  uint64_t offset;
  uint64_t size;
  LocationT()
      : offset(0),
        size(0) {
  }
};

struct Location FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef LocationT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NAME = 4,
    VT_OFFSET = 6,
    VT_SIZE = 8
  };
  const flatbuffers::String *name() const {
    return GetPointer<const flatbuffers::String *>(VT_NAME);
  }
  flatbuffers::String *mutable_name() {
    return GetPointer<flatbuffers::String *>(VT_NAME);
  }
  uint64_t offset() const {
    return GetField<uint64_t>(VT_OFFSET, 0);
  }
  bool mutate_offset(uint64_t _offset) {
    return SetField<uint64_t>(VT_OFFSET, _offset, 0);
  }
  uint64_t size() const {
    return GetField<uint64_t>(VT_SIZE, 0);
  }
  bool mutate_size(uint64_t _size) {
    return SetField<uint64_t>(VT_SIZE, _size, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<uint64_t>(verifier, VT_OFFSET) &&
           VerifyField<uint64_t>(verifier, VT_SIZE) &&
           verifier.EndTable();
  }
  LocationT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(LocationT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Location> Pack(flatbuffers::FlatBufferBuilder &_fbb, const LocationT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct LocationBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_name(flatbuffers::Offset<flatbuffers::String> name) {
    fbb_.AddOffset(Location::VT_NAME, name);
  }
  void add_offset(uint64_t offset) {
    fbb_.AddElement<uint64_t>(Location::VT_OFFSET, offset, 0);
  }
  void add_size(uint64_t size) {
    fbb_.AddElement<uint64_t>(Location::VT_SIZE, size, 0);
  }
  explicit LocationBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  LocationBuilder &operator=(const LocationBuilder &);
  flatbuffers::Offset<Location> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Location>(end);
    fbb_.Required(o, Location::VT_NAME);
    return o;
  }
};

inline flatbuffers::Offset<Location> CreateLocation(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::String> name = 0,
    uint64_t offset = 0,
    uint64_t size = 0) {
  LocationBuilder builder_(_fbb);
  builder_.add_size(size);
  builder_.add_offset(offset);
  builder_.add_name(name);
  return builder_.Finish();
}

inline flatbuffers::Offset<Location> CreateLocationDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const char *name = nullptr,
    uint64_t offset = 0,
    uint64_t size = 0) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return bmodel::CreateLocation(
      _fbb,
      name__,
      offset,
      size);
}

flatbuffers::Offset<Location> CreateLocation(flatbuffers::FlatBufferBuilder &_fbb, const LocationT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CoeffMemT : public flatbuffers::NativeTable {
  typedef CoeffMem TableType;
  uint64_t address;
  std::vector<uint8_t> check_code;
  std::unique_ptr<Binary> binary_coeff;
  std::vector<std::unique_ptr<LocationT>> location;
  int32_t encrypt_mode;
  uint64_t decrypt_size;
  CoeffMemT()
      : address(0),
        encrypt_mode(0),
        decrypt_size(0) {
  }
};

struct CoeffMem FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CoeffMemT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ADDRESS = 4,
    VT_CHECK_CODE = 6,
    VT_BINARY_COEFF = 8,
    VT_LOCATION = 10,
    VT_ENCRYPT_MODE = 12,
    VT_DECRYPT_SIZE = 14
  };
  uint64_t address() const {
    return GetField<uint64_t>(VT_ADDRESS, 0);
  }
  bool mutate_address(uint64_t _address) {
    return SetField<uint64_t>(VT_ADDRESS, _address, 0);
  }
  const flatbuffers::Vector<uint8_t> *check_code() const {
    return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_CHECK_CODE);
  }
  flatbuffers::Vector<uint8_t> *mutable_check_code() {
    return GetPointer<flatbuffers::Vector<uint8_t> *>(VT_CHECK_CODE);
  }
  const Binary *binary_coeff() const {
    return GetStruct<const Binary *>(VT_BINARY_COEFF);
  }
  Binary *mutable_binary_coeff() {
    return GetStruct<Binary *>(VT_BINARY_COEFF);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Location>> *location() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Location>> *>(VT_LOCATION);
  }
  flatbuffers::Vector<flatbuffers::Offset<Location>> *mutable_location() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Location>> *>(VT_LOCATION);
  }
  int32_t encrypt_mode() const {
    return GetField<int32_t>(VT_ENCRYPT_MODE, 0);
  }
  bool mutate_encrypt_mode(int32_t _encrypt_mode) {
    return SetField<int32_t>(VT_ENCRYPT_MODE, _encrypt_mode, 0);
  }
  uint64_t decrypt_size() const {
    return GetField<uint64_t>(VT_DECRYPT_SIZE, 0);
  }
  bool mutate_decrypt_size(uint64_t _decrypt_size) {
    return SetField<uint64_t>(VT_DECRYPT_SIZE, _decrypt_size, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_ADDRESS) &&
           VerifyOffset(verifier, VT_CHECK_CODE) &&
           verifier.VerifyVector(check_code()) &&
           VerifyField<Binary>(verifier, VT_BINARY_COEFF) &&
           VerifyOffset(verifier, VT_LOCATION) &&
           verifier.VerifyVector(location()) &&
           verifier.VerifyVectorOfTables(location()) &&
           VerifyField<int32_t>(verifier, VT_ENCRYPT_MODE) &&
           VerifyField<uint64_t>(verifier, VT_DECRYPT_SIZE) &&
           verifier.EndTable();
  }
  CoeffMemT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CoeffMemT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<CoeffMem> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CoeffMemT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CoeffMemBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_address(uint64_t address) {
    fbb_.AddElement<uint64_t>(CoeffMem::VT_ADDRESS, address, 0);
  }
  void add_check_code(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> check_code) {
    fbb_.AddOffset(CoeffMem::VT_CHECK_CODE, check_code);
  }
  void add_binary_coeff(const Binary *binary_coeff) {
    fbb_.AddStruct(CoeffMem::VT_BINARY_COEFF, binary_coeff);
  }
  void add_location(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Location>>> location) {
    fbb_.AddOffset(CoeffMem::VT_LOCATION, location);
  }
  void add_encrypt_mode(int32_t encrypt_mode) {
    fbb_.AddElement<int32_t>(CoeffMem::VT_ENCRYPT_MODE, encrypt_mode, 0);
  }
  void add_decrypt_size(uint64_t decrypt_size) {
    fbb_.AddElement<uint64_t>(CoeffMem::VT_DECRYPT_SIZE, decrypt_size, 0);
  }
  explicit CoeffMemBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CoeffMemBuilder &operator=(const CoeffMemBuilder &);
  flatbuffers::Offset<CoeffMem> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CoeffMem>(end);
    return o;
  }
};

inline flatbuffers::Offset<CoeffMem> CreateCoeffMem(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t address = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint8_t>> check_code = 0,
    const Binary *binary_coeff = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Location>>> location = 0,
    int32_t encrypt_mode = 0,
    uint64_t decrypt_size = 0) {
  CoeffMemBuilder builder_(_fbb);
  builder_.add_decrypt_size(decrypt_size);
  builder_.add_address(address);
  builder_.add_encrypt_mode(encrypt_mode);
  builder_.add_location(location);
  builder_.add_binary_coeff(binary_coeff);
  builder_.add_check_code(check_code);
  return builder_.Finish();
}

inline flatbuffers::Offset<CoeffMem> CreateCoeffMemDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t address = 0,
    const std::vector<uint8_t> *check_code = nullptr,
    const Binary *binary_coeff = 0,
    const std::vector<flatbuffers::Offset<Location>> *location = nullptr,
    int32_t encrypt_mode = 0,
    uint64_t decrypt_size = 0) {
  auto check_code__ = check_code ? _fbb.CreateVector<uint8_t>(*check_code) : 0;
  auto location__ = location ? _fbb.CreateVector<flatbuffers::Offset<Location>>(*location) : 0;
  return bmodel::CreateCoeffMem(
      _fbb,
      address,
      check_code__,
      binary_coeff,
      location__,
      encrypt_mode,
      decrypt_size);
}

flatbuffers::Offset<CoeffMem> CreateCoeffMem(flatbuffers::FlatBufferBuilder &_fbb, const CoeffMemT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorT : public flatbuffers::NativeTable {
  typedef Tensor TableType;
  std::string name;
  uint32_t data_type;
  int32_t gmem_stmode;
  uint64_t device_addr;
  uint64_t size;
  std::vector<std::unique_ptr<ShapeT>> shape;
  uint32_t mem_type;
  float scale;
  uint32_t cpu_addr;
  uint32_t pad_h;
  int32_t zero_point;
  int32_t hidden;
  int32_t index;
  TensorT()
      : data_type(0),
        gmem_stmode(0),
        device_addr(0),
        size(0),
        mem_type(0),
        scale(1.0f),
        cpu_addr(0),
        pad_h(0),
        zero_point(0),
        hidden(0),
        index(0) {
  }
};

struct Tensor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef TensorT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NAME = 4,
    VT_DATA_TYPE = 6,
    VT_GMEM_STMODE = 8,
    VT_DEVICE_ADDR = 10,
    VT_SIZE = 12,
    VT_SHAPE = 14,
    VT_MEM_TYPE = 16,
    VT_SCALE = 18,
    VT_CPU_ADDR = 20,
    VT_PAD_H = 22,
    VT_ZERO_POINT = 24,
    VT_HIDDEN = 26,
    VT_INDEX = 28
  };
  const flatbuffers::String *name() const {
    return GetPointer<const flatbuffers::String *>(VT_NAME);
  }
  flatbuffers::String *mutable_name() {
    return GetPointer<flatbuffers::String *>(VT_NAME);
  }
  uint32_t data_type() const {
    return GetField<uint32_t>(VT_DATA_TYPE, 0);
  }
  bool mutate_data_type(uint32_t _data_type) {
    return SetField<uint32_t>(VT_DATA_TYPE, _data_type, 0);
  }
  int32_t gmem_stmode() const {
    return GetField<int32_t>(VT_GMEM_STMODE, 0);
  }
  bool mutate_gmem_stmode(int32_t _gmem_stmode) {
    return SetField<int32_t>(VT_GMEM_STMODE, _gmem_stmode, 0);
  }
  uint64_t device_addr() const {
    return GetField<uint64_t>(VT_DEVICE_ADDR, 0);
  }
  bool mutate_device_addr(uint64_t _device_addr) {
    return SetField<uint64_t>(VT_DEVICE_ADDR, _device_addr, 0);
  }
  uint64_t size() const {
    return GetField<uint64_t>(VT_SIZE, 0);
  }
  bool mutate_size(uint64_t _size) {
    return SetField<uint64_t>(VT_SIZE, _size, 0);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Shape>> *shape() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Shape>> *>(VT_SHAPE);
  }
  flatbuffers::Vector<flatbuffers::Offset<Shape>> *mutable_shape() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Shape>> *>(VT_SHAPE);
  }
  uint32_t mem_type() const {
    return GetField<uint32_t>(VT_MEM_TYPE, 0);
  }
  bool mutate_mem_type(uint32_t _mem_type) {
    return SetField<uint32_t>(VT_MEM_TYPE, _mem_type, 0);
  }
  float scale() const {
    return GetField<float>(VT_SCALE, 1.0f);
  }
  bool mutate_scale(float _scale) {
    return SetField<float>(VT_SCALE, _scale, 1.0f);
  }
  uint32_t cpu_addr() const {
    return GetField<uint32_t>(VT_CPU_ADDR, 0);
  }
  bool mutate_cpu_addr(uint32_t _cpu_addr) {
    return SetField<uint32_t>(VT_CPU_ADDR, _cpu_addr, 0);
  }
  uint32_t pad_h() const {
    return GetField<uint32_t>(VT_PAD_H, 0);
  }
  bool mutate_pad_h(uint32_t _pad_h) {
    return SetField<uint32_t>(VT_PAD_H, _pad_h, 0);
  }
  int32_t zero_point() const {
    return GetField<int32_t>(VT_ZERO_POINT, 0);
  }
  bool mutate_zero_point(int32_t _zero_point) {
    return SetField<int32_t>(VT_ZERO_POINT, _zero_point, 0);
  }
  int32_t hidden() const {
    return GetField<int32_t>(VT_HIDDEN, 0);
  }
  bool mutate_hidden(int32_t _hidden) {
    return SetField<int32_t>(VT_HIDDEN, _hidden, 0);
  }
  int32_t index() const {
    return GetField<int32_t>(VT_INDEX, 0);
  }
  bool mutate_index(int32_t _index) {
    return SetField<int32_t>(VT_INDEX, _index, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<uint32_t>(verifier, VT_DATA_TYPE) &&
           VerifyField<int32_t>(verifier, VT_GMEM_STMODE) &&
           VerifyField<uint64_t>(verifier, VT_DEVICE_ADDR) &&
           VerifyField<uint64_t>(verifier, VT_SIZE) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           verifier.VerifyVectorOfTables(shape()) &&
           VerifyField<uint32_t>(verifier, VT_MEM_TYPE) &&
           VerifyField<float>(verifier, VT_SCALE) &&
           VerifyField<uint32_t>(verifier, VT_CPU_ADDR) &&
           VerifyField<uint32_t>(verifier, VT_PAD_H) &&
           VerifyField<int32_t>(verifier, VT_ZERO_POINT) &&
           VerifyField<int32_t>(verifier, VT_HIDDEN) &&
           VerifyField<int32_t>(verifier, VT_INDEX) &&
           verifier.EndTable();
  }
  TensorT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Tensor> Pack(flatbuffers::FlatBufferBuilder &_fbb, const TensorT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_name(flatbuffers::Offset<flatbuffers::String> name) {
    fbb_.AddOffset(Tensor::VT_NAME, name);
  }
  void add_data_type(uint32_t data_type) {
    fbb_.AddElement<uint32_t>(Tensor::VT_DATA_TYPE, data_type, 0);
  }
  void add_gmem_stmode(int32_t gmem_stmode) {
    fbb_.AddElement<int32_t>(Tensor::VT_GMEM_STMODE, gmem_stmode, 0);
  }
  void add_device_addr(uint64_t device_addr) {
    fbb_.AddElement<uint64_t>(Tensor::VT_DEVICE_ADDR, device_addr, 0);
  }
  void add_size(uint64_t size) {
    fbb_.AddElement<uint64_t>(Tensor::VT_SIZE, size, 0);
  }
  void add_shape(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Shape>>> shape) {
    fbb_.AddOffset(Tensor::VT_SHAPE, shape);
  }
  void add_mem_type(uint32_t mem_type) {
    fbb_.AddElement<uint32_t>(Tensor::VT_MEM_TYPE, mem_type, 0);
  }
  void add_scale(float scale) {
    fbb_.AddElement<float>(Tensor::VT_SCALE, scale, 1.0f);
  }
  void add_cpu_addr(uint32_t cpu_addr) {
    fbb_.AddElement<uint32_t>(Tensor::VT_CPU_ADDR, cpu_addr, 0);
  }
  void add_pad_h(uint32_t pad_h) {
    fbb_.AddElement<uint32_t>(Tensor::VT_PAD_H, pad_h, 0);
  }
  void add_zero_point(int32_t zero_point) {
    fbb_.AddElement<int32_t>(Tensor::VT_ZERO_POINT, zero_point, 0);
  }
  void add_hidden(int32_t hidden) {
    fbb_.AddElement<int32_t>(Tensor::VT_HIDDEN, hidden, 0);
  }
  void add_index(int32_t index) {
    fbb_.AddElement<int32_t>(Tensor::VT_INDEX, index, 0);
  }
  explicit TensorBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  TensorBuilder &operator=(const TensorBuilder &);
  flatbuffers::Offset<Tensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Tensor>(end);
    fbb_.Required(o, Tensor::VT_NAME);
    return o;
  }
};

inline flatbuffers::Offset<Tensor> CreateTensor(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::String> name = 0,
    uint32_t data_type = 0,
    int32_t gmem_stmode = 0,
    uint64_t device_addr = 0,
    uint64_t size = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Shape>>> shape = 0,
    uint32_t mem_type = 0,
    float scale = 1.0f,
    uint32_t cpu_addr = 0,
    uint32_t pad_h = 0,
    int32_t zero_point = 0,
    int32_t hidden = 0,
    int32_t index = 0) {
  TensorBuilder builder_(_fbb);
  builder_.add_size(size);
  builder_.add_device_addr(device_addr);
  builder_.add_index(index);
  builder_.add_hidden(hidden);
  builder_.add_zero_point(zero_point);
  builder_.add_pad_h(pad_h);
  builder_.add_cpu_addr(cpu_addr);
  builder_.add_scale(scale);
  builder_.add_mem_type(mem_type);
  builder_.add_shape(shape);
  builder_.add_gmem_stmode(gmem_stmode);
  builder_.add_data_type(data_type);
  builder_.add_name(name);
  return builder_.Finish();
}

inline flatbuffers::Offset<Tensor> CreateTensorDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const char *name = nullptr,
    uint32_t data_type = 0,
    int32_t gmem_stmode = 0,
    uint64_t device_addr = 0,
    uint64_t size = 0,
    const std::vector<flatbuffers::Offset<Shape>> *shape = nullptr,
    uint32_t mem_type = 0,
    float scale = 1.0f,
    uint32_t cpu_addr = 0,
    uint32_t pad_h = 0,
    int32_t zero_point = 0,
    int32_t hidden = 0,
    int32_t index = 0) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  auto shape__ = shape ? _fbb.CreateVector<flatbuffers::Offset<Shape>>(*shape) : 0;
  return bmodel::CreateTensor(
      _fbb,
      name__,
      data_type,
      gmem_stmode,
      device_addr,
      size,
      shape__,
      mem_type,
      scale,
      cpu_addr,
      pad_h,
      zero_point,
      hidden,
      index);
}

flatbuffers::Offset<Tensor> CreateTensor(flatbuffers::FlatBufferBuilder &_fbb, const TensorT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CpuConstT : public flatbuffers::NativeTable {
  typedef CpuConst TableType;
  std::string name;
  std::unique_ptr<Binary> const_data;
  std::vector<uint8_t> check_code;
  CpuConstT() {
  }
};

struct CpuConst FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CpuConstT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NAME = 4,
    VT_CONST_DATA = 6,
    VT_CHECK_CODE = 8
  };
  const flatbuffers::String *name() const {
    return GetPointer<const flatbuffers::String *>(VT_NAME);
  }
  flatbuffers::String *mutable_name() {
    return GetPointer<flatbuffers::String *>(VT_NAME);
  }
  const Binary *const_data() const {
    return GetStruct<const Binary *>(VT_CONST_DATA);
  }
  Binary *mutable_const_data() {
    return GetStruct<Binary *>(VT_CONST_DATA);
  }
  const flatbuffers::Vector<uint8_t> *check_code() const {
    return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_CHECK_CODE);
  }
  flatbuffers::Vector<uint8_t> *mutable_check_code() {
    return GetPointer<flatbuffers::Vector<uint8_t> *>(VT_CHECK_CODE);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<Binary>(verifier, VT_CONST_DATA) &&
           VerifyOffset(verifier, VT_CHECK_CODE) &&
           verifier.VerifyVector(check_code()) &&
           verifier.EndTable();
  }
  CpuConstT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CpuConstT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<CpuConst> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CpuConstT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CpuConstBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_name(flatbuffers::Offset<flatbuffers::String> name) {
    fbb_.AddOffset(CpuConst::VT_NAME, name);
  }
  void add_const_data(const Binary *const_data) {
    fbb_.AddStruct(CpuConst::VT_CONST_DATA, const_data);
  }
  void add_check_code(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> check_code) {
    fbb_.AddOffset(CpuConst::VT_CHECK_CODE, check_code);
  }
  explicit CpuConstBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CpuConstBuilder &operator=(const CpuConstBuilder &);
  flatbuffers::Offset<CpuConst> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CpuConst>(end);
    return o;
  }
};

inline flatbuffers::Offset<CpuConst> CreateCpuConst(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::String> name = 0,
    const Binary *const_data = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint8_t>> check_code = 0) {
  CpuConstBuilder builder_(_fbb);
  builder_.add_check_code(check_code);
  builder_.add_const_data(const_data);
  builder_.add_name(name);
  return builder_.Finish();
}

inline flatbuffers::Offset<CpuConst> CreateCpuConstDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const char *name = nullptr,
    const Binary *const_data = 0,
    const std::vector<uint8_t> *check_code = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  auto check_code__ = check_code ? _fbb.CreateVector<uint8_t>(*check_code) : 0;
  return bmodel::CreateCpuConst(
      _fbb,
      name__,
      const_data,
      check_code__);
}

flatbuffers::Offset<CpuConst> CreateCpuConst(flatbuffers::FlatBufferBuilder &_fbb, const CpuConstT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CpuParamT : public flatbuffers::NativeTable {
  typedef CpuParam TableType;
  int32_t op_type;
  std::unique_ptr<Binary> binary_param;
  std::vector<std::unique_ptr<CpuConstT>> cpu_const;
  int32_t is_custom;
  CpuParamT()
      : op_type(0),
        is_custom(0) {
  }
};

struct CpuParam FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CpuParamT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OP_TYPE = 4,
    VT_BINARY_PARAM = 6,
    VT_CPU_CONST = 8,
    VT_IS_CUSTOM = 10
  };
  int32_t op_type() const {
    return GetField<int32_t>(VT_OP_TYPE, 0);
  }
  bool mutate_op_type(int32_t _op_type) {
    return SetField<int32_t>(VT_OP_TYPE, _op_type, 0);
  }
  const Binary *binary_param() const {
    return GetStruct<const Binary *>(VT_BINARY_PARAM);
  }
  Binary *mutable_binary_param() {
    return GetStruct<Binary *>(VT_BINARY_PARAM);
  }
  const flatbuffers::Vector<flatbuffers::Offset<CpuConst>> *cpu_const() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<CpuConst>> *>(VT_CPU_CONST);
  }
  flatbuffers::Vector<flatbuffers::Offset<CpuConst>> *mutable_cpu_const() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<CpuConst>> *>(VT_CPU_CONST);
  }
  int32_t is_custom() const {
    return GetField<int32_t>(VT_IS_CUSTOM, 0);
  }
  bool mutate_is_custom(int32_t _is_custom) {
    return SetField<int32_t>(VT_IS_CUSTOM, _is_custom, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_OP_TYPE) &&
           VerifyField<Binary>(verifier, VT_BINARY_PARAM) &&
           VerifyOffset(verifier, VT_CPU_CONST) &&
           verifier.VerifyVector(cpu_const()) &&
           verifier.VerifyVectorOfTables(cpu_const()) &&
           VerifyField<int32_t>(verifier, VT_IS_CUSTOM) &&
           verifier.EndTable();
  }
  CpuParamT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CpuParamT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<CpuParam> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CpuParamT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CpuParamBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_op_type(int32_t op_type) {
    fbb_.AddElement<int32_t>(CpuParam::VT_OP_TYPE, op_type, 0);
  }
  void add_binary_param(const Binary *binary_param) {
    fbb_.AddStruct(CpuParam::VT_BINARY_PARAM, binary_param);
  }
  void add_cpu_const(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CpuConst>>> cpu_const) {
    fbb_.AddOffset(CpuParam::VT_CPU_CONST, cpu_const);
  }
  void add_is_custom(int32_t is_custom) {
    fbb_.AddElement<int32_t>(CpuParam::VT_IS_CUSTOM, is_custom, 0);
  }
  explicit CpuParamBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CpuParamBuilder &operator=(const CpuParamBuilder &);
  flatbuffers::Offset<CpuParam> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CpuParam>(end);
    return o;
  }
};

inline flatbuffers::Offset<CpuParam> CreateCpuParam(
    flatbuffers::FlatBufferBuilder &_fbb,
    int32_t op_type = 0,
    const Binary *binary_param = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CpuConst>>> cpu_const = 0,
    int32_t is_custom = 0) {
  CpuParamBuilder builder_(_fbb);
  builder_.add_is_custom(is_custom);
  builder_.add_cpu_const(cpu_const);
  builder_.add_binary_param(binary_param);
  builder_.add_op_type(op_type);
  return builder_.Finish();
}

inline flatbuffers::Offset<CpuParam> CreateCpuParamDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    int32_t op_type = 0,
    const Binary *binary_param = 0,
    const std::vector<flatbuffers::Offset<CpuConst>> *cpu_const = nullptr,
    int32_t is_custom = 0) {
  auto cpu_const__ = cpu_const ? _fbb.CreateVector<flatbuffers::Offset<CpuConst>>(*cpu_const) : 0;
  return bmodel::CreateCpuParam(
      _fbb,
      op_type,
      binary_param,
      cpu_const__,
      is_custom);
}

flatbuffers::Offset<CpuParam> CreateCpuParam(flatbuffers::FlatBufferBuilder &_fbb, const CpuParamT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct OutputFromT : public flatbuffers::NativeTable {
  typedef OutputFrom TableType;
  std::vector<int32_t> indice;
  OutputFromT() {
  }
};

struct OutputFrom FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef OutputFromT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INDICE = 4
  };
  const flatbuffers::Vector<int32_t> *indice() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_INDICE);
  }
  flatbuffers::Vector<int32_t> *mutable_indice() {
    return GetPointer<flatbuffers::Vector<int32_t> *>(VT_INDICE);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INDICE) &&
           verifier.VerifyVector(indice()) &&
           verifier.EndTable();
  }
  OutputFromT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(OutputFromT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<OutputFrom> Pack(flatbuffers::FlatBufferBuilder &_fbb, const OutputFromT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct OutputFromBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_indice(flatbuffers::Offset<flatbuffers::Vector<int32_t>> indice) {
    fbb_.AddOffset(OutputFrom::VT_INDICE, indice);
  }
  explicit OutputFromBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  OutputFromBuilder &operator=(const OutputFromBuilder &);
  flatbuffers::Offset<OutputFrom> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<OutputFrom>(end);
    return o;
  }
};

inline flatbuffers::Offset<OutputFrom> CreateOutputFrom(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<int32_t>> indice = 0) {
  OutputFromBuilder builder_(_fbb);
  builder_.add_indice(indice);
  return builder_.Finish();
}

inline flatbuffers::Offset<OutputFrom> CreateOutputFromDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *indice = nullptr) {
  auto indice__ = indice ? _fbb.CreateVector<int32_t>(*indice) : 0;
  return bmodel::CreateOutputFrom(
      _fbb,
      indice__);
}

flatbuffers::Offset<OutputFrom> CreateOutputFrom(flatbuffers::FlatBufferBuilder &_fbb, const OutputFromT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct MergeParamT : public flatbuffers::NativeTable {
  typedef MergeParam TableType;
  std::vector<std::unique_ptr<OutputFromT>> output_from;
  MergeParamT() {
  }
};

struct MergeParam FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef MergeParamT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_FROM = 4
  };
  const flatbuffers::Vector<flatbuffers::Offset<OutputFrom>> *output_from() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<OutputFrom>> *>(VT_OUTPUT_FROM);
  }
  flatbuffers::Vector<flatbuffers::Offset<OutputFrom>> *mutable_output_from() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<OutputFrom>> *>(VT_OUTPUT_FROM);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_OUTPUT_FROM) &&
           verifier.VerifyVector(output_from()) &&
           verifier.VerifyVectorOfTables(output_from()) &&
           verifier.EndTable();
  }
  MergeParamT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(MergeParamT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<MergeParam> Pack(flatbuffers::FlatBufferBuilder &_fbb, const MergeParamT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct MergeParamBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_output_from(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<OutputFrom>>> output_from) {
    fbb_.AddOffset(MergeParam::VT_OUTPUT_FROM, output_from);
  }
  explicit MergeParamBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  MergeParamBuilder &operator=(const MergeParamBuilder &);
  flatbuffers::Offset<MergeParam> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<MergeParam>(end);
    return o;
  }
};

inline flatbuffers::Offset<MergeParam> CreateMergeParam(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<OutputFrom>>> output_from = 0) {
  MergeParamBuilder builder_(_fbb);
  builder_.add_output_from(output_from);
  return builder_.Finish();
}

inline flatbuffers::Offset<MergeParam> CreateMergeParamDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<flatbuffers::Offset<OutputFrom>> *output_from = nullptr) {
  auto output_from__ = output_from ? _fbb.CreateVector<flatbuffers::Offset<OutputFrom>>(*output_from) : 0;
  return bmodel::CreateMergeParam(
      _fbb,
      output_from__);
}

flatbuffers::Offset<MergeParam> CreateMergeParam(flatbuffers::FlatBufferBuilder &_fbb, const MergeParamT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SwitchParamT : public flatbuffers::NativeTable {
  typedef SwitchParam TableType;
  std::vector<int32_t> output_from;
  std::vector<int32_t> output_branch;
  SwitchParamT() {
  }
};

struct SwitchParam FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef SwitchParamT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_FROM = 4,
    VT_OUTPUT_BRANCH = 6
  };
  const flatbuffers::Vector<int32_t> *output_from() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_OUTPUT_FROM);
  }
  flatbuffers::Vector<int32_t> *mutable_output_from() {
    return GetPointer<flatbuffers::Vector<int32_t> *>(VT_OUTPUT_FROM);
  }
  const flatbuffers::Vector<int32_t> *output_branch() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_OUTPUT_BRANCH);
  }
  flatbuffers::Vector<int32_t> *mutable_output_branch() {
    return GetPointer<flatbuffers::Vector<int32_t> *>(VT_OUTPUT_BRANCH);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_OUTPUT_FROM) &&
           verifier.VerifyVector(output_from()) &&
           VerifyOffset(verifier, VT_OUTPUT_BRANCH) &&
           verifier.VerifyVector(output_branch()) &&
           verifier.EndTable();
  }
  SwitchParamT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SwitchParamT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<SwitchParam> Pack(flatbuffers::FlatBufferBuilder &_fbb, const SwitchParamT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SwitchParamBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_output_from(flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_from) {
    fbb_.AddOffset(SwitchParam::VT_OUTPUT_FROM, output_from);
  }
  void add_output_branch(flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_branch) {
    fbb_.AddOffset(SwitchParam::VT_OUTPUT_BRANCH, output_branch);
  }
  explicit SwitchParamBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  SwitchParamBuilder &operator=(const SwitchParamBuilder &);
  flatbuffers::Offset<SwitchParam> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<SwitchParam>(end);
    return o;
  }
};

inline flatbuffers::Offset<SwitchParam> CreateSwitchParam(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_from = 0,
    flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_branch = 0) {
  SwitchParamBuilder builder_(_fbb);
  builder_.add_output_branch(output_branch);
  builder_.add_output_from(output_from);
  return builder_.Finish();
}

inline flatbuffers::Offset<SwitchParam> CreateSwitchParamDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *output_from = nullptr,
    const std::vector<int32_t> *output_branch = nullptr) {
  auto output_from__ = output_from ? _fbb.CreateVector<int32_t>(*output_from) : 0;
  auto output_branch__ = output_branch ? _fbb.CreateVector<int32_t>(*output_branch) : 0;
  return bmodel::CreateSwitchParam(
      _fbb,
      output_from__,
      output_branch__);
}

flatbuffers::Offset<SwitchParam> CreateSwitchParam(flatbuffers::FlatBufferBuilder &_fbb, const SwitchParamT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SubNetT : public flatbuffers::NativeTable {
  typedef SubNet TableType;
  int32_t subnet_mode;
  std::vector<std::unique_ptr<CmdGroupT>> cmd_group;
  std::vector<std::unique_ptr<CpuParamT>> cpu_param;
  std::vector<std::unique_ptr<TensorT>> input_tensor;
  std::vector<std::unique_ptr<TensorT>> output_tensor;
  int32_t is_dynamic;
  uint32_t ir_offset;
  uint32_t ir_len;
  int32_t n_dynamic;
  int32_t h_w_dynamic;
  int32_t id;
  std::vector<int32_t> next_subnet_ids;
  std::unique_ptr<MergeParamT> merge_param;
  std::unique_ptr<SwitchParamT> switch_param;
  std::vector<std::unique_ptr<CoreCommandsT>> core_commands;
  SubNetT()
      : subnet_mode(0),
        is_dynamic(0),
        ir_offset(0),
        ir_len(0),
        n_dynamic(0),
        h_w_dynamic(0),
        id(-1) {
  }
};

struct SubNet FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef SubNetT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SUBNET_MODE = 4,
    VT_CMD_GROUP = 6,
    VT_CPU_PARAM = 8,
    VT_INPUT_TENSOR = 10,
    VT_OUTPUT_TENSOR = 12,
    VT_IS_DYNAMIC = 14,
    VT_IR_OFFSET = 16,
    VT_IR_LEN = 18,
    VT_N_DYNAMIC = 20,
    VT_H_W_DYNAMIC = 22,
    VT_ID = 24,
    VT_NEXT_SUBNET_IDS = 26,
    VT_MERGE_PARAM = 28,
    VT_SWITCH_PARAM = 30,
    VT_CORE_COMMANDS = 32
  };
  int32_t subnet_mode() const {
    return GetField<int32_t>(VT_SUBNET_MODE, 0);
  }
  bool mutate_subnet_mode(int32_t _subnet_mode) {
    return SetField<int32_t>(VT_SUBNET_MODE, _subnet_mode, 0);
  }
  const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *cmd_group() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_CMD_GROUP);
  }
  flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *mutable_cmd_group() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_CMD_GROUP);
  }
  const flatbuffers::Vector<flatbuffers::Offset<CpuParam>> *cpu_param() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<CpuParam>> *>(VT_CPU_PARAM);
  }
  flatbuffers::Vector<flatbuffers::Offset<CpuParam>> *mutable_cpu_param() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<CpuParam>> *>(VT_CPU_PARAM);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *input_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_input_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *output_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_output_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  int32_t is_dynamic() const {
    return GetField<int32_t>(VT_IS_DYNAMIC, 0);
  }
  bool mutate_is_dynamic(int32_t _is_dynamic) {
    return SetField<int32_t>(VT_IS_DYNAMIC, _is_dynamic, 0);
  }
  uint32_t ir_offset() const {
    return GetField<uint32_t>(VT_IR_OFFSET, 0);
  }
  bool mutate_ir_offset(uint32_t _ir_offset) {
    return SetField<uint32_t>(VT_IR_OFFSET, _ir_offset, 0);
  }
  uint32_t ir_len() const {
    return GetField<uint32_t>(VT_IR_LEN, 0);
  }
  bool mutate_ir_len(uint32_t _ir_len) {
    return SetField<uint32_t>(VT_IR_LEN, _ir_len, 0);
  }
  int32_t n_dynamic() const {
    return GetField<int32_t>(VT_N_DYNAMIC, 0);
  }
  bool mutate_n_dynamic(int32_t _n_dynamic) {
    return SetField<int32_t>(VT_N_DYNAMIC, _n_dynamic, 0);
  }
  int32_t h_w_dynamic() const {
    return GetField<int32_t>(VT_H_W_DYNAMIC, 0);
  }
  bool mutate_h_w_dynamic(int32_t _h_w_dynamic) {
    return SetField<int32_t>(VT_H_W_DYNAMIC, _h_w_dynamic, 0);
  }
  int32_t id() const {
    return GetField<int32_t>(VT_ID, -1);
  }
  bool mutate_id(int32_t _id) {
    return SetField<int32_t>(VT_ID, _id, -1);
  }
  const flatbuffers::Vector<int32_t> *next_subnet_ids() const {
    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_NEXT_SUBNET_IDS);
  }
  flatbuffers::Vector<int32_t> *mutable_next_subnet_ids() {
    return GetPointer<flatbuffers::Vector<int32_t> *>(VT_NEXT_SUBNET_IDS);
  }
  const MergeParam *merge_param() const {
    return GetPointer<const MergeParam *>(VT_MERGE_PARAM);
  }
  MergeParam *mutable_merge_param() {
    return GetPointer<MergeParam *>(VT_MERGE_PARAM);
  }
  const SwitchParam *switch_param() const {
    return GetPointer<const SwitchParam *>(VT_SWITCH_PARAM);
  }
  SwitchParam *mutable_switch_param() {
    return GetPointer<SwitchParam *>(VT_SWITCH_PARAM);
  }
  const flatbuffers::Vector<flatbuffers::Offset<CoreCommands>> *core_commands() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<CoreCommands>> *>(VT_CORE_COMMANDS);
  }
  flatbuffers::Vector<flatbuffers::Offset<CoreCommands>> *mutable_core_commands() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<CoreCommands>> *>(VT_CORE_COMMANDS);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_SUBNET_MODE) &&
           VerifyOffset(verifier, VT_CMD_GROUP) &&
           verifier.VerifyVector(cmd_group()) &&
           verifier.VerifyVectorOfTables(cmd_group()) &&
           VerifyOffset(verifier, VT_CPU_PARAM) &&
           verifier.VerifyVector(cpu_param()) &&
           verifier.VerifyVectorOfTables(cpu_param()) &&
           VerifyOffset(verifier, VT_INPUT_TENSOR) &&
           verifier.VerifyVector(input_tensor()) &&
           verifier.VerifyVectorOfTables(input_tensor()) &&
           VerifyOffset(verifier, VT_OUTPUT_TENSOR) &&
           verifier.VerifyVector(output_tensor()) &&
           verifier.VerifyVectorOfTables(output_tensor()) &&
           VerifyField<int32_t>(verifier, VT_IS_DYNAMIC) &&
           VerifyField<uint32_t>(verifier, VT_IR_OFFSET) &&
           VerifyField<uint32_t>(verifier, VT_IR_LEN) &&
           VerifyField<int32_t>(verifier, VT_N_DYNAMIC) &&
           VerifyField<int32_t>(verifier, VT_H_W_DYNAMIC) &&
           VerifyField<int32_t>(verifier, VT_ID) &&
           VerifyOffset(verifier, VT_NEXT_SUBNET_IDS) &&
           verifier.VerifyVector(next_subnet_ids()) &&
           VerifyOffset(verifier, VT_MERGE_PARAM) &&
           verifier.VerifyTable(merge_param()) &&
           VerifyOffset(verifier, VT_SWITCH_PARAM) &&
           verifier.VerifyTable(switch_param()) &&
           VerifyOffset(verifier, VT_CORE_COMMANDS) &&
           verifier.VerifyVector(core_commands()) &&
           verifier.VerifyVectorOfTables(core_commands()) &&
           verifier.EndTable();
  }
  SubNetT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SubNetT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<SubNet> Pack(flatbuffers::FlatBufferBuilder &_fbb, const SubNetT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SubNetBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_subnet_mode(int32_t subnet_mode) {
    fbb_.AddElement<int32_t>(SubNet::VT_SUBNET_MODE, subnet_mode, 0);
  }
  void add_cmd_group(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> cmd_group) {
    fbb_.AddOffset(SubNet::VT_CMD_GROUP, cmd_group);
  }
  void add_cpu_param(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CpuParam>>> cpu_param) {
    fbb_.AddOffset(SubNet::VT_CPU_PARAM, cpu_param);
  }
  void add_input_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor) {
    fbb_.AddOffset(SubNet::VT_INPUT_TENSOR, input_tensor);
  }
  void add_output_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor) {
    fbb_.AddOffset(SubNet::VT_OUTPUT_TENSOR, output_tensor);
  }
  void add_is_dynamic(int32_t is_dynamic) {
    fbb_.AddElement<int32_t>(SubNet::VT_IS_DYNAMIC, is_dynamic, 0);
  }
  void add_ir_offset(uint32_t ir_offset) {
    fbb_.AddElement<uint32_t>(SubNet::VT_IR_OFFSET, ir_offset, 0);
  }
  void add_ir_len(uint32_t ir_len) {
    fbb_.AddElement<uint32_t>(SubNet::VT_IR_LEN, ir_len, 0);
  }
  void add_n_dynamic(int32_t n_dynamic) {
    fbb_.AddElement<int32_t>(SubNet::VT_N_DYNAMIC, n_dynamic, 0);
  }
  void add_h_w_dynamic(int32_t h_w_dynamic) {
    fbb_.AddElement<int32_t>(SubNet::VT_H_W_DYNAMIC, h_w_dynamic, 0);
  }
  void add_id(int32_t id) {
    fbb_.AddElement<int32_t>(SubNet::VT_ID, id, -1);
  }
  void add_next_subnet_ids(flatbuffers::Offset<flatbuffers::Vector<int32_t>> next_subnet_ids) {
    fbb_.AddOffset(SubNet::VT_NEXT_SUBNET_IDS, next_subnet_ids);
  }
  void add_merge_param(flatbuffers::Offset<MergeParam> merge_param) {
    fbb_.AddOffset(SubNet::VT_MERGE_PARAM, merge_param);
  }
  void add_switch_param(flatbuffers::Offset<SwitchParam> switch_param) {
    fbb_.AddOffset(SubNet::VT_SWITCH_PARAM, switch_param);
  }
  void add_core_commands(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CoreCommands>>> core_commands) {
    fbb_.AddOffset(SubNet::VT_CORE_COMMANDS, core_commands);
  }
  explicit SubNetBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  SubNetBuilder &operator=(const SubNetBuilder &);
  flatbuffers::Offset<SubNet> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<SubNet>(end);
    return o;
  }
};

inline flatbuffers::Offset<SubNet> CreateSubNet(
    flatbuffers::FlatBufferBuilder &_fbb,
    int32_t subnet_mode = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> cmd_group = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CpuParam>>> cpu_param = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor = 0,
    int32_t is_dynamic = 0,
    uint32_t ir_offset = 0,
    uint32_t ir_len = 0,
    int32_t n_dynamic = 0,
    int32_t h_w_dynamic = 0,
    int32_t id = -1,
    flatbuffers::Offset<flatbuffers::Vector<int32_t>> next_subnet_ids = 0,
    flatbuffers::Offset<MergeParam> merge_param = 0,
    flatbuffers::Offset<SwitchParam> switch_param = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CoreCommands>>> core_commands = 0) {
  SubNetBuilder builder_(_fbb);
  builder_.add_core_commands(core_commands);
  builder_.add_switch_param(switch_param);
  builder_.add_merge_param(merge_param);
  builder_.add_next_subnet_ids(next_subnet_ids);
  builder_.add_id(id);
  builder_.add_h_w_dynamic(h_w_dynamic);
  builder_.add_n_dynamic(n_dynamic);
  builder_.add_ir_len(ir_len);
  builder_.add_ir_offset(ir_offset);
  builder_.add_is_dynamic(is_dynamic);
  builder_.add_output_tensor(output_tensor);
  builder_.add_input_tensor(input_tensor);
  builder_.add_cpu_param(cpu_param);
  builder_.add_cmd_group(cmd_group);
  builder_.add_subnet_mode(subnet_mode);
  return builder_.Finish();
}

inline flatbuffers::Offset<SubNet> CreateSubNetDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    int32_t subnet_mode = 0,
    const std::vector<flatbuffers::Offset<CmdGroup>> *cmd_group = nullptr,
    const std::vector<flatbuffers::Offset<CpuParam>> *cpu_param = nullptr,
    const std::vector<flatbuffers::Offset<Tensor>> *input_tensor = nullptr,
    const std::vector<flatbuffers::Offset<Tensor>> *output_tensor = nullptr,
    int32_t is_dynamic = 0,
    uint32_t ir_offset = 0,
    uint32_t ir_len = 0,
    int32_t n_dynamic = 0,
    int32_t h_w_dynamic = 0,
    int32_t id = -1,
    const std::vector<int32_t> *next_subnet_ids = nullptr,
    flatbuffers::Offset<MergeParam> merge_param = 0,
    flatbuffers::Offset<SwitchParam> switch_param = 0,
    const std::vector<flatbuffers::Offset<CoreCommands>> *core_commands = nullptr) {
  auto cmd_group__ = cmd_group ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>>(*cmd_group) : 0;
  auto cpu_param__ = cpu_param ? _fbb.CreateVector<flatbuffers::Offset<CpuParam>>(*cpu_param) : 0;
  auto input_tensor__ = input_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*input_tensor) : 0;
  auto output_tensor__ = output_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*output_tensor) : 0;
  auto next_subnet_ids__ = next_subnet_ids ? _fbb.CreateVector<int32_t>(*next_subnet_ids) : 0;
  auto core_commands__ = core_commands ? _fbb.CreateVector<flatbuffers::Offset<CoreCommands>>(*core_commands) : 0;
  return bmodel::CreateSubNet(
      _fbb,
      subnet_mode,
      cmd_group__,
      cpu_param__,
      input_tensor__,
      output_tensor__,
      is_dynamic,
      ir_offset,
      ir_len,
      n_dynamic,
      h_w_dynamic,
      id,
      next_subnet_ids__,
      merge_param,
      switch_param,
      core_commands__);
}

flatbuffers::Offset<SubNet> CreateSubNet(flatbuffers::FlatBufferBuilder &_fbb, const SubNetT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NetStaticT : public flatbuffers::NativeTable {
  typedef NetStatic TableType;
  std::vector<std::unique_ptr<TensorT>> input_tensor;
  std::vector<std::unique_ptr<TensorT>> output_tensor;
  std::vector<std::unique_ptr<CmdGroupT>> cmd_group;
  uint64_t ctx_addr;
  uint64_t ctx_size;
  std::unique_ptr<CoeffMemT> coeff_mem;
  std::vector<std::unique_ptr<SubNetT>> sub_net;
  std::unique_ptr<Binary> net_profile;
  NetStaticT()
      : ctx_addr(0),
        ctx_size(0) {
  }
};

struct NetStatic FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NetStaticT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUT_TENSOR = 4,
    VT_OUTPUT_TENSOR = 6,
    VT_CMD_GROUP = 8,
    VT_CTX_ADDR = 10,
    VT_CTX_SIZE = 12,
    VT_COEFF_MEM = 14,
    VT_SUB_NET = 16,
    VT_NET_PROFILE = 18
  };
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *input_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_input_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *output_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_output_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *cmd_group() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_CMD_GROUP);
  }
  flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *mutable_cmd_group() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_CMD_GROUP);
  }
  uint64_t ctx_addr() const {
    return GetField<uint64_t>(VT_CTX_ADDR, 0);
  }
  bool mutate_ctx_addr(uint64_t _ctx_addr) {
    return SetField<uint64_t>(VT_CTX_ADDR, _ctx_addr, 0);
  }
  uint64_t ctx_size() const {
    return GetField<uint64_t>(VT_CTX_SIZE, 0);
  }
  bool mutate_ctx_size(uint64_t _ctx_size) {
    return SetField<uint64_t>(VT_CTX_SIZE, _ctx_size, 0);
  }
  const CoeffMem *coeff_mem() const {
    return GetPointer<const CoeffMem *>(VT_COEFF_MEM);
  }
  CoeffMem *mutable_coeff_mem() {
    return GetPointer<CoeffMem *>(VT_COEFF_MEM);
  }
  const flatbuffers::Vector<flatbuffers::Offset<SubNet>> *sub_net() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<SubNet>> *>(VT_SUB_NET);
  }
  flatbuffers::Vector<flatbuffers::Offset<SubNet>> *mutable_sub_net() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<SubNet>> *>(VT_SUB_NET);
  }
  const Binary *net_profile() const {
    return GetStruct<const Binary *>(VT_NET_PROFILE);
  }
  Binary *mutable_net_profile() {
    return GetStruct<Binary *>(VT_NET_PROFILE);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_INPUT_TENSOR) &&
           verifier.VerifyVector(input_tensor()) &&
           verifier.VerifyVectorOfTables(input_tensor()) &&
           VerifyOffsetRequired(verifier, VT_OUTPUT_TENSOR) &&
           verifier.VerifyVector(output_tensor()) &&
           verifier.VerifyVectorOfTables(output_tensor()) &&
           VerifyOffset(verifier, VT_CMD_GROUP) &&
           verifier.VerifyVector(cmd_group()) &&
           verifier.VerifyVectorOfTables(cmd_group()) &&
           VerifyField<uint64_t>(verifier, VT_CTX_ADDR) &&
           VerifyField<uint64_t>(verifier, VT_CTX_SIZE) &&
           VerifyOffset(verifier, VT_COEFF_MEM) &&
           verifier.VerifyTable(coeff_mem()) &&
           VerifyOffset(verifier, VT_SUB_NET) &&
           verifier.VerifyVector(sub_net()) &&
           verifier.VerifyVectorOfTables(sub_net()) &&
           VerifyField<Binary>(verifier, VT_NET_PROFILE) &&
           verifier.EndTable();
  }
  NetStaticT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NetStaticT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<NetStatic> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetStaticT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NetStaticBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_input_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor) {
    fbb_.AddOffset(NetStatic::VT_INPUT_TENSOR, input_tensor);
  }
  void add_output_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor) {
    fbb_.AddOffset(NetStatic::VT_OUTPUT_TENSOR, output_tensor);
  }
  void add_cmd_group(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> cmd_group) {
    fbb_.AddOffset(NetStatic::VT_CMD_GROUP, cmd_group);
  }
  void add_ctx_addr(uint64_t ctx_addr) {
    fbb_.AddElement<uint64_t>(NetStatic::VT_CTX_ADDR, ctx_addr, 0);
  }
  void add_ctx_size(uint64_t ctx_size) {
    fbb_.AddElement<uint64_t>(NetStatic::VT_CTX_SIZE, ctx_size, 0);
  }
  void add_coeff_mem(flatbuffers::Offset<CoeffMem> coeff_mem) {
    fbb_.AddOffset(NetStatic::VT_COEFF_MEM, coeff_mem);
  }
  void add_sub_net(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<SubNet>>> sub_net) {
    fbb_.AddOffset(NetStatic::VT_SUB_NET, sub_net);
  }
  void add_net_profile(const Binary *net_profile) {
    fbb_.AddStruct(NetStatic::VT_NET_PROFILE, net_profile);
  }
  explicit NetStaticBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NetStaticBuilder &operator=(const NetStaticBuilder &);
  flatbuffers::Offset<NetStatic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NetStatic>(end);
    fbb_.Required(o, NetStatic::VT_INPUT_TENSOR);
    fbb_.Required(o, NetStatic::VT_OUTPUT_TENSOR);
    return o;
  }
};

inline flatbuffers::Offset<NetStatic> CreateNetStatic(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> cmd_group = 0,
    uint64_t ctx_addr = 0,
    uint64_t ctx_size = 0,
    flatbuffers::Offset<CoeffMem> coeff_mem = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<SubNet>>> sub_net = 0,
    const Binary *net_profile = 0) {
  NetStaticBuilder builder_(_fbb);
  builder_.add_ctx_size(ctx_size);
  builder_.add_ctx_addr(ctx_addr);
  builder_.add_net_profile(net_profile);
  builder_.add_sub_net(sub_net);
  builder_.add_coeff_mem(coeff_mem);
  builder_.add_cmd_group(cmd_group);
  builder_.add_output_tensor(output_tensor);
  builder_.add_input_tensor(input_tensor);
  return builder_.Finish();
}

inline flatbuffers::Offset<NetStatic> CreateNetStaticDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<flatbuffers::Offset<Tensor>> *input_tensor = nullptr,
    const std::vector<flatbuffers::Offset<Tensor>> *output_tensor = nullptr,
    const std::vector<flatbuffers::Offset<CmdGroup>> *cmd_group = nullptr,
    uint64_t ctx_addr = 0,
    uint64_t ctx_size = 0,
    flatbuffers::Offset<CoeffMem> coeff_mem = 0,
    const std::vector<flatbuffers::Offset<SubNet>> *sub_net = nullptr,
    const Binary *net_profile = 0) {
  auto input_tensor__ = input_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*input_tensor) : 0;
  auto output_tensor__ = output_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*output_tensor) : 0;
  auto cmd_group__ = cmd_group ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>>(*cmd_group) : 0;
  auto sub_net__ = sub_net ? _fbb.CreateVector<flatbuffers::Offset<SubNet>>(*sub_net) : 0;
  return bmodel::CreateNetStatic(
      _fbb,
      input_tensor__,
      output_tensor__,
      cmd_group__,
      ctx_addr,
      ctx_size,
      coeff_mem,
      sub_net__,
      net_profile);
}

flatbuffers::Offset<NetStatic> CreateNetStatic(flatbuffers::FlatBufferBuilder &_fbb, const NetStaticT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NetDynamicT : public flatbuffers::NativeTable {
  typedef NetDynamic TableType;
  std::vector<std::unique_ptr<TensorT>> input_tensor;
  std::vector<std::unique_ptr<TensorT>> output_tensor;
  int32_t n_dynamic;
  int32_t h_w_dynamic;
  std::vector<std::unique_ptr<StageIRT>> stage_ir;
  std::unique_ptr<Binary> binary_ir;
  uint64_t ctx_addr;
  uint64_t ctx_size;
  std::unique_ptr<CoeffMemT> coeff_mem;
  std::vector<std::unique_ptr<SubNetT>> sub_net;
  NetDynamicT()
      : n_dynamic(0),
        h_w_dynamic(0),
        ctx_addr(0),
        ctx_size(0) {
  }
};

struct NetDynamic FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NetDynamicT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUT_TENSOR = 4,
    VT_OUTPUT_TENSOR = 6,
    VT_N_DYNAMIC = 8,
    VT_H_W_DYNAMIC = 10,
    VT_STAGE_IR = 12,
    VT_BINARY_IR = 14,
    VT_CTX_ADDR = 16,
    VT_CTX_SIZE = 18,
    VT_COEFF_MEM = 20,
    VT_SUB_NET = 22
  };
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *input_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_input_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *output_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_output_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  int32_t n_dynamic() const {
    return GetField<int32_t>(VT_N_DYNAMIC, 0);
  }
  bool mutate_n_dynamic(int32_t _n_dynamic) {
    return SetField<int32_t>(VT_N_DYNAMIC, _n_dynamic, 0);
  }
  int32_t h_w_dynamic() const {
    return GetField<int32_t>(VT_H_W_DYNAMIC, 0);
  }
  bool mutate_h_w_dynamic(int32_t _h_w_dynamic) {
    return SetField<int32_t>(VT_H_W_DYNAMIC, _h_w_dynamic, 0);
  }
  const flatbuffers::Vector<flatbuffers::Offset<StageIR>> *stage_ir() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<StageIR>> *>(VT_STAGE_IR);
  }
  flatbuffers::Vector<flatbuffers::Offset<StageIR>> *mutable_stage_ir() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<StageIR>> *>(VT_STAGE_IR);
  }
  const Binary *binary_ir() const {
    return GetStruct<const Binary *>(VT_BINARY_IR);
  }
  Binary *mutable_binary_ir() {
    return GetStruct<Binary *>(VT_BINARY_IR);
  }
  uint64_t ctx_addr() const {
    return GetField<uint64_t>(VT_CTX_ADDR, 0);
  }
  bool mutate_ctx_addr(uint64_t _ctx_addr) {
    return SetField<uint64_t>(VT_CTX_ADDR, _ctx_addr, 0);
  }
  uint64_t ctx_size() const {
    return GetField<uint64_t>(VT_CTX_SIZE, 0);
  }
  bool mutate_ctx_size(uint64_t _ctx_size) {
    return SetField<uint64_t>(VT_CTX_SIZE, _ctx_size, 0);
  }
  const CoeffMem *coeff_mem() const {
    return GetPointer<const CoeffMem *>(VT_COEFF_MEM);
  }
  CoeffMem *mutable_coeff_mem() {
    return GetPointer<CoeffMem *>(VT_COEFF_MEM);
  }
  const flatbuffers::Vector<flatbuffers::Offset<SubNet>> *sub_net() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<SubNet>> *>(VT_SUB_NET);
  }
  flatbuffers::Vector<flatbuffers::Offset<SubNet>> *mutable_sub_net() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<SubNet>> *>(VT_SUB_NET);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_INPUT_TENSOR) &&
           verifier.VerifyVector(input_tensor()) &&
           verifier.VerifyVectorOfTables(input_tensor()) &&
           VerifyOffsetRequired(verifier, VT_OUTPUT_TENSOR) &&
           verifier.VerifyVector(output_tensor()) &&
           verifier.VerifyVectorOfTables(output_tensor()) &&
           VerifyField<int32_t>(verifier, VT_N_DYNAMIC) &&
           VerifyField<int32_t>(verifier, VT_H_W_DYNAMIC) &&
           VerifyOffset(verifier, VT_STAGE_IR) &&
           verifier.VerifyVector(stage_ir()) &&
           verifier.VerifyVectorOfTables(stage_ir()) &&
           VerifyField<Binary>(verifier, VT_BINARY_IR) &&
           VerifyField<uint64_t>(verifier, VT_CTX_ADDR) &&
           VerifyField<uint64_t>(verifier, VT_CTX_SIZE) &&
           VerifyOffset(verifier, VT_COEFF_MEM) &&
           verifier.VerifyTable(coeff_mem()) &&
           VerifyOffset(verifier, VT_SUB_NET) &&
           verifier.VerifyVector(sub_net()) &&
           verifier.VerifyVectorOfTables(sub_net()) &&
           verifier.EndTable();
  }
  NetDynamicT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NetDynamicT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<NetDynamic> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetDynamicT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NetDynamicBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_input_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor) {
    fbb_.AddOffset(NetDynamic::VT_INPUT_TENSOR, input_tensor);
  }
  void add_output_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor) {
    fbb_.AddOffset(NetDynamic::VT_OUTPUT_TENSOR, output_tensor);
  }
  void add_n_dynamic(int32_t n_dynamic) {
    fbb_.AddElement<int32_t>(NetDynamic::VT_N_DYNAMIC, n_dynamic, 0);
  }
  void add_h_w_dynamic(int32_t h_w_dynamic) {
    fbb_.AddElement<int32_t>(NetDynamic::VT_H_W_DYNAMIC, h_w_dynamic, 0);
  }
  void add_stage_ir(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<StageIR>>> stage_ir) {
    fbb_.AddOffset(NetDynamic::VT_STAGE_IR, stage_ir);
  }
  void add_binary_ir(const Binary *binary_ir) {
    fbb_.AddStruct(NetDynamic::VT_BINARY_IR, binary_ir);
  }
  void add_ctx_addr(uint64_t ctx_addr) {
    fbb_.AddElement<uint64_t>(NetDynamic::VT_CTX_ADDR, ctx_addr, 0);
  }
  void add_ctx_size(uint64_t ctx_size) {
    fbb_.AddElement<uint64_t>(NetDynamic::VT_CTX_SIZE, ctx_size, 0);
  }
  void add_coeff_mem(flatbuffers::Offset<CoeffMem> coeff_mem) {
    fbb_.AddOffset(NetDynamic::VT_COEFF_MEM, coeff_mem);
  }
  void add_sub_net(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<SubNet>>> sub_net) {
    fbb_.AddOffset(NetDynamic::VT_SUB_NET, sub_net);
  }
  explicit NetDynamicBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NetDynamicBuilder &operator=(const NetDynamicBuilder &);
  flatbuffers::Offset<NetDynamic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NetDynamic>(end);
    fbb_.Required(o, NetDynamic::VT_INPUT_TENSOR);
    fbb_.Required(o, NetDynamic::VT_OUTPUT_TENSOR);
    return o;
  }
};

inline flatbuffers::Offset<NetDynamic> CreateNetDynamic(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor = 0,
    int32_t n_dynamic = 0,
    int32_t h_w_dynamic = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<StageIR>>> stage_ir = 0,
    const Binary *binary_ir = 0,
    uint64_t ctx_addr = 0,
    uint64_t ctx_size = 0,
    flatbuffers::Offset<CoeffMem> coeff_mem = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<SubNet>>> sub_net = 0) {
  NetDynamicBuilder builder_(_fbb);
  builder_.add_ctx_size(ctx_size);
  builder_.add_ctx_addr(ctx_addr);
  builder_.add_sub_net(sub_net);
  builder_.add_coeff_mem(coeff_mem);
  builder_.add_binary_ir(binary_ir);
  builder_.add_stage_ir(stage_ir);
  builder_.add_h_w_dynamic(h_w_dynamic);
  builder_.add_n_dynamic(n_dynamic);
  builder_.add_output_tensor(output_tensor);
  builder_.add_input_tensor(input_tensor);
  return builder_.Finish();
}

inline flatbuffers::Offset<NetDynamic> CreateNetDynamicDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<flatbuffers::Offset<Tensor>> *input_tensor = nullptr,
    const std::vector<flatbuffers::Offset<Tensor>> *output_tensor = nullptr,
    int32_t n_dynamic = 0,
    int32_t h_w_dynamic = 0,
    const std::vector<flatbuffers::Offset<StageIR>> *stage_ir = nullptr,
    const Binary *binary_ir = 0,
    uint64_t ctx_addr = 0,
    uint64_t ctx_size = 0,
    flatbuffers::Offset<CoeffMem> coeff_mem = 0,
    const std::vector<flatbuffers::Offset<SubNet>> *sub_net = nullptr) {
  auto input_tensor__ = input_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*input_tensor) : 0;
  auto output_tensor__ = output_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*output_tensor) : 0;
  auto stage_ir__ = stage_ir ? _fbb.CreateVector<flatbuffers::Offset<StageIR>>(*stage_ir) : 0;
  auto sub_net__ = sub_net ? _fbb.CreateVector<flatbuffers::Offset<SubNet>>(*sub_net) : 0;
  return bmodel::CreateNetDynamic(
      _fbb,
      input_tensor__,
      output_tensor__,
      n_dynamic,
      h_w_dynamic,
      stage_ir__,
      binary_ir,
      ctx_addr,
      ctx_size,
      coeff_mem,
      sub_net__);
}

flatbuffers::Offset<NetDynamic> CreateNetDynamic(flatbuffers::FlatBufferBuilder &_fbb, const NetDynamicT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NetParameterT : public flatbuffers::NativeTable {
  typedef NetParameter TableType;
  std::vector<std::unique_ptr<TensorT>> input_tensor;
  std::vector<std::unique_ptr<TensorT>> output_tensor;
  uint64_t ctx_addr;
  uint64_t ctx_size;
  std::unique_ptr<CoeffMemT> coeff_mem;
  int32_t is_dynamic;
  int32_t n_dynamic;
  int32_t h_w_dynamic;
  std::vector<std::unique_ptr<CmdGroupT>> cmd_group;
  std::unique_ptr<Binary> net_profile;
  std::vector<std::unique_ptr<StageIRT>> stage_ir;
  std::unique_ptr<Binary> binary_ir;
  std::vector<std::unique_ptr<SubNetT>> sub_net;
  uint32_t cpu_mem_size;
  std::vector<uint64_t> ctx_sizes;
  std::unique_ptr<Binary> net_stat;
  uint32_t core_num;
  uint64_t io_addr;
  uint64_t io_size;
  std::unique_ptr<Binary> tensor_loc;
  uint64_t dynamic_ctx_addr;
  uint64_t dynamic_coeff_offset;
  uint64_t dynamic_combined_coeff_offset;
  NetParameterT()
      : ctx_addr(0),
        ctx_size(0),
        is_dynamic(0),
        n_dynamic(0),
        h_w_dynamic(0),
        cpu_mem_size(0),
        core_num(0),
        io_addr(0),
        io_size(0),
        dynamic_ctx_addr(0),
        dynamic_coeff_offset(0),
        dynamic_combined_coeff_offset(0) {
  }
};

struct NetParameter FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NetParameterT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUT_TENSOR = 4,
    VT_OUTPUT_TENSOR = 6,
    VT_CTX_ADDR = 8,
    VT_CTX_SIZE = 10,
    VT_COEFF_MEM = 12,
    VT_IS_DYNAMIC = 14,
    VT_N_DYNAMIC = 16,
    VT_H_W_DYNAMIC = 18,
    VT_CMD_GROUP = 20,
    VT_NET_PROFILE = 22,
    VT_STAGE_IR = 24,
    VT_BINARY_IR = 26,
    VT_SUB_NET = 28,
    VT_CPU_MEM_SIZE = 30,
    VT_CTX_SIZES = 32,
    VT_NET_STAT = 34,
    VT_CORE_NUM = 36,
    VT_IO_ADDR = 38,
    VT_IO_SIZE = 40,
    VT_TENSOR_LOC = 42,
    VT_DYNAMIC_CTX_ADDR = 44,
    VT_DYNAMIC_COEFF_OFFSET = 46,
    VT_DYNAMIC_COMBINED_COEFF_OFFSET = 48
  };
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *input_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_input_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_INPUT_TENSOR);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *output_tensor() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  flatbuffers::Vector<flatbuffers::Offset<Tensor>> *mutable_output_tensor() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Tensor>> *>(VT_OUTPUT_TENSOR);
  }
  uint64_t ctx_addr() const {
    return GetField<uint64_t>(VT_CTX_ADDR, 0);
  }
  bool mutate_ctx_addr(uint64_t _ctx_addr) {
    return SetField<uint64_t>(VT_CTX_ADDR, _ctx_addr, 0);
  }
  uint64_t ctx_size() const {
    return GetField<uint64_t>(VT_CTX_SIZE, 0);
  }
  bool mutate_ctx_size(uint64_t _ctx_size) {
    return SetField<uint64_t>(VT_CTX_SIZE, _ctx_size, 0);
  }
  const CoeffMem *coeff_mem() const {
    return GetPointer<const CoeffMem *>(VT_COEFF_MEM);
  }
  CoeffMem *mutable_coeff_mem() {
    return GetPointer<CoeffMem *>(VT_COEFF_MEM);
  }
  int32_t is_dynamic() const {
    return GetField<int32_t>(VT_IS_DYNAMIC, 0);
  }
  bool mutate_is_dynamic(int32_t _is_dynamic) {
    return SetField<int32_t>(VT_IS_DYNAMIC, _is_dynamic, 0);
  }
  int32_t n_dynamic() const {
    return GetField<int32_t>(VT_N_DYNAMIC, 0);
  }
  bool mutate_n_dynamic(int32_t _n_dynamic) {
    return SetField<int32_t>(VT_N_DYNAMIC, _n_dynamic, 0);
  }
  int32_t h_w_dynamic() const {
    return GetField<int32_t>(VT_H_W_DYNAMIC, 0);
  }
  bool mutate_h_w_dynamic(int32_t _h_w_dynamic) {
    return SetField<int32_t>(VT_H_W_DYNAMIC, _h_w_dynamic, 0);
  }
  const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *cmd_group() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_CMD_GROUP);
  }
  flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *mutable_cmd_group() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>> *>(VT_CMD_GROUP);
  }
  const Binary *net_profile() const {
    return GetStruct<const Binary *>(VT_NET_PROFILE);
  }
  Binary *mutable_net_profile() {
    return GetStruct<Binary *>(VT_NET_PROFILE);
  }
  const flatbuffers::Vector<flatbuffers::Offset<StageIR>> *stage_ir() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<StageIR>> *>(VT_STAGE_IR);
  }
  flatbuffers::Vector<flatbuffers::Offset<StageIR>> *mutable_stage_ir() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<StageIR>> *>(VT_STAGE_IR);
  }
  const Binary *binary_ir() const {
    return GetStruct<const Binary *>(VT_BINARY_IR);
  }
  Binary *mutable_binary_ir() {
    return GetStruct<Binary *>(VT_BINARY_IR);
  }
  const flatbuffers::Vector<flatbuffers::Offset<SubNet>> *sub_net() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<SubNet>> *>(VT_SUB_NET);
  }
  flatbuffers::Vector<flatbuffers::Offset<SubNet>> *mutable_sub_net() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<SubNet>> *>(VT_SUB_NET);
  }
  uint32_t cpu_mem_size() const {
    return GetField<uint32_t>(VT_CPU_MEM_SIZE, 0);
  }
  bool mutate_cpu_mem_size(uint32_t _cpu_mem_size) {
    return SetField<uint32_t>(VT_CPU_MEM_SIZE, _cpu_mem_size, 0);
  }
  const flatbuffers::Vector<uint64_t> *ctx_sizes() const {
    return GetPointer<const flatbuffers::Vector<uint64_t> *>(VT_CTX_SIZES);
  }
  flatbuffers::Vector<uint64_t> *mutable_ctx_sizes() {
    return GetPointer<flatbuffers::Vector<uint64_t> *>(VT_CTX_SIZES);
  }
  const Binary *net_stat() const {
    return GetStruct<const Binary *>(VT_NET_STAT);
  }
  Binary *mutable_net_stat() {
    return GetStruct<Binary *>(VT_NET_STAT);
  }
  uint32_t core_num() const {
    return GetField<uint32_t>(VT_CORE_NUM, 0);
  }
  bool mutate_core_num(uint32_t _core_num) {
    return SetField<uint32_t>(VT_CORE_NUM, _core_num, 0);
  }
  uint64_t io_addr() const {
    return GetField<uint64_t>(VT_IO_ADDR, 0);
  }
  bool mutate_io_addr(uint64_t _io_addr) {
    return SetField<uint64_t>(VT_IO_ADDR, _io_addr, 0);
  }
  uint64_t io_size() const {
    return GetField<uint64_t>(VT_IO_SIZE, 0);
  }
  bool mutate_io_size(uint64_t _io_size) {
    return SetField<uint64_t>(VT_IO_SIZE, _io_size, 0);
  }
  const Binary *tensor_loc() const {
    return GetStruct<const Binary *>(VT_TENSOR_LOC);
  }
  Binary *mutable_tensor_loc() {
    return GetStruct<Binary *>(VT_TENSOR_LOC);
  }
  uint64_t dynamic_ctx_addr() const {
    return GetField<uint64_t>(VT_DYNAMIC_CTX_ADDR, 0);
  }
  bool mutate_dynamic_ctx_addr(uint64_t _dynamic_ctx_addr) {
    return SetField<uint64_t>(VT_DYNAMIC_CTX_ADDR, _dynamic_ctx_addr, 0);
  }
  uint64_t dynamic_coeff_offset() const {
    return GetField<uint64_t>(VT_DYNAMIC_COEFF_OFFSET, 0);
  }
  bool mutate_dynamic_coeff_offset(uint64_t _dynamic_coeff_offset) {
    return SetField<uint64_t>(VT_DYNAMIC_COEFF_OFFSET, _dynamic_coeff_offset, 0);
  }
  uint64_t dynamic_combined_coeff_offset() const {
    return GetField<uint64_t>(VT_DYNAMIC_COMBINED_COEFF_OFFSET, 0);
  }
  bool mutate_dynamic_combined_coeff_offset(uint64_t _dynamic_combined_coeff_offset) {
    return SetField<uint64_t>(VT_DYNAMIC_COMBINED_COEFF_OFFSET, _dynamic_combined_coeff_offset, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_INPUT_TENSOR) &&
           verifier.VerifyVector(input_tensor()) &&
           verifier.VerifyVectorOfTables(input_tensor()) &&
           VerifyOffsetRequired(verifier, VT_OUTPUT_TENSOR) &&
           verifier.VerifyVector(output_tensor()) &&
           verifier.VerifyVectorOfTables(output_tensor()) &&
           VerifyField<uint64_t>(verifier, VT_CTX_ADDR) &&
           VerifyField<uint64_t>(verifier, VT_CTX_SIZE) &&
           VerifyOffset(verifier, VT_COEFF_MEM) &&
           verifier.VerifyTable(coeff_mem()) &&
           VerifyField<int32_t>(verifier, VT_IS_DYNAMIC) &&
           VerifyField<int32_t>(verifier, VT_N_DYNAMIC) &&
           VerifyField<int32_t>(verifier, VT_H_W_DYNAMIC) &&
           VerifyOffset(verifier, VT_CMD_GROUP) &&
           verifier.VerifyVector(cmd_group()) &&
           verifier.VerifyVectorOfTables(cmd_group()) &&
           VerifyField<Binary>(verifier, VT_NET_PROFILE) &&
           VerifyOffset(verifier, VT_STAGE_IR) &&
           verifier.VerifyVector(stage_ir()) &&
           verifier.VerifyVectorOfTables(stage_ir()) &&
           VerifyField<Binary>(verifier, VT_BINARY_IR) &&
           VerifyOffset(verifier, VT_SUB_NET) &&
           verifier.VerifyVector(sub_net()) &&
           verifier.VerifyVectorOfTables(sub_net()) &&
           VerifyField<uint32_t>(verifier, VT_CPU_MEM_SIZE) &&
           VerifyOffset(verifier, VT_CTX_SIZES) &&
           verifier.VerifyVector(ctx_sizes()) &&
           VerifyField<Binary>(verifier, VT_NET_STAT) &&
           VerifyField<uint32_t>(verifier, VT_CORE_NUM) &&
           VerifyField<uint64_t>(verifier, VT_IO_ADDR) &&
           VerifyField<uint64_t>(verifier, VT_IO_SIZE) &&
           VerifyField<Binary>(verifier, VT_TENSOR_LOC) &&
           VerifyField<uint64_t>(verifier, VT_DYNAMIC_CTX_ADDR) &&
           VerifyField<uint64_t>(verifier, VT_DYNAMIC_COEFF_OFFSET) &&
           VerifyField<uint64_t>(verifier, VT_DYNAMIC_COMBINED_COEFF_OFFSET) &&
           verifier.EndTable();
  }
  NetParameterT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NetParameterT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<NetParameter> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetParameterT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NetParameterBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_input_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor) {
    fbb_.AddOffset(NetParameter::VT_INPUT_TENSOR, input_tensor);
  }
  void add_output_tensor(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor) {
    fbb_.AddOffset(NetParameter::VT_OUTPUT_TENSOR, output_tensor);
  }
  void add_ctx_addr(uint64_t ctx_addr) {
    fbb_.AddElement<uint64_t>(NetParameter::VT_CTX_ADDR, ctx_addr, 0);
  }
  void add_ctx_size(uint64_t ctx_size) {
    fbb_.AddElement<uint64_t>(NetParameter::VT_CTX_SIZE, ctx_size, 0);
  }
  void add_coeff_mem(flatbuffers::Offset<CoeffMem> coeff_mem) {
    fbb_.AddOffset(NetParameter::VT_COEFF_MEM, coeff_mem);
  }
  void add_is_dynamic(int32_t is_dynamic) {
    fbb_.AddElement<int32_t>(NetParameter::VT_IS_DYNAMIC, is_dynamic, 0);
  }
  void add_n_dynamic(int32_t n_dynamic) {
    fbb_.AddElement<int32_t>(NetParameter::VT_N_DYNAMIC, n_dynamic, 0);
  }
  void add_h_w_dynamic(int32_t h_w_dynamic) {
    fbb_.AddElement<int32_t>(NetParameter::VT_H_W_DYNAMIC, h_w_dynamic, 0);
  }
  void add_cmd_group(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> cmd_group) {
    fbb_.AddOffset(NetParameter::VT_CMD_GROUP, cmd_group);
  }
  void add_net_profile(const Binary *net_profile) {
    fbb_.AddStruct(NetParameter::VT_NET_PROFILE, net_profile);
  }
  void add_stage_ir(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<StageIR>>> stage_ir) {
    fbb_.AddOffset(NetParameter::VT_STAGE_IR, stage_ir);
  }
  void add_binary_ir(const Binary *binary_ir) {
    fbb_.AddStruct(NetParameter::VT_BINARY_IR, binary_ir);
  }
  void add_sub_net(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<SubNet>>> sub_net) {
    fbb_.AddOffset(NetParameter::VT_SUB_NET, sub_net);
  }
  void add_cpu_mem_size(uint32_t cpu_mem_size) {
    fbb_.AddElement<uint32_t>(NetParameter::VT_CPU_MEM_SIZE, cpu_mem_size, 0);
  }
  void add_ctx_sizes(flatbuffers::Offset<flatbuffers::Vector<uint64_t>> ctx_sizes) {
    fbb_.AddOffset(NetParameter::VT_CTX_SIZES, ctx_sizes);
  }
  void add_net_stat(const Binary *net_stat) {
    fbb_.AddStruct(NetParameter::VT_NET_STAT, net_stat);
  }
  void add_core_num(uint32_t core_num) {
    fbb_.AddElement<uint32_t>(NetParameter::VT_CORE_NUM, core_num, 0);
  }
  void add_io_addr(uint64_t io_addr) {
    fbb_.AddElement<uint64_t>(NetParameter::VT_IO_ADDR, io_addr, 0);
  }
  void add_io_size(uint64_t io_size) {
    fbb_.AddElement<uint64_t>(NetParameter::VT_IO_SIZE, io_size, 0);
  }
  void add_tensor_loc(const Binary *tensor_loc) {
    fbb_.AddStruct(NetParameter::VT_TENSOR_LOC, tensor_loc);
  }
  void add_dynamic_ctx_addr(uint64_t dynamic_ctx_addr) {
    fbb_.AddElement<uint64_t>(NetParameter::VT_DYNAMIC_CTX_ADDR, dynamic_ctx_addr, 0);
  }
  void add_dynamic_coeff_offset(uint64_t dynamic_coeff_offset) {
    fbb_.AddElement<uint64_t>(NetParameter::VT_DYNAMIC_COEFF_OFFSET, dynamic_coeff_offset, 0);
  }
  void add_dynamic_combined_coeff_offset(uint64_t dynamic_combined_coeff_offset) {
    fbb_.AddElement<uint64_t>(NetParameter::VT_DYNAMIC_COMBINED_COEFF_OFFSET, dynamic_combined_coeff_offset, 0);
  }
  explicit NetParameterBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NetParameterBuilder &operator=(const NetParameterBuilder &);
  flatbuffers::Offset<NetParameter> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NetParameter>(end);
    fbb_.Required(o, NetParameter::VT_INPUT_TENSOR);
    fbb_.Required(o, NetParameter::VT_OUTPUT_TENSOR);
    return o;
  }
};

inline flatbuffers::Offset<NetParameter> CreateNetParameter(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> input_tensor = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Tensor>>> output_tensor = 0,
    uint64_t ctx_addr = 0,
    uint64_t ctx_size = 0,
    flatbuffers::Offset<CoeffMem> coeff_mem = 0,
    int32_t is_dynamic = 0,
    int32_t n_dynamic = 0,
    int32_t h_w_dynamic = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<CmdGroup>>> cmd_group = 0,
    const Binary *net_profile = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<StageIR>>> stage_ir = 0,
    const Binary *binary_ir = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<SubNet>>> sub_net = 0,
    uint32_t cpu_mem_size = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint64_t>> ctx_sizes = 0,
    const Binary *net_stat = 0,
    uint32_t core_num = 0,
    uint64_t io_addr = 0,
    uint64_t io_size = 0,
    const Binary *tensor_loc = 0,
    uint64_t dynamic_ctx_addr = 0,
    uint64_t dynamic_coeff_offset = 0,
    uint64_t dynamic_combined_coeff_offset = 0) {
  NetParameterBuilder builder_(_fbb);
  builder_.add_dynamic_combined_coeff_offset(dynamic_combined_coeff_offset);
  builder_.add_dynamic_coeff_offset(dynamic_coeff_offset);
  builder_.add_dynamic_ctx_addr(dynamic_ctx_addr);
  builder_.add_io_size(io_size);
  builder_.add_io_addr(io_addr);
  builder_.add_ctx_size(ctx_size);
  builder_.add_ctx_addr(ctx_addr);
  builder_.add_tensor_loc(tensor_loc);
  builder_.add_core_num(core_num);
  builder_.add_net_stat(net_stat);
  builder_.add_ctx_sizes(ctx_sizes);
  builder_.add_cpu_mem_size(cpu_mem_size);
  builder_.add_sub_net(sub_net);
  builder_.add_binary_ir(binary_ir);
  builder_.add_stage_ir(stage_ir);
  builder_.add_net_profile(net_profile);
  builder_.add_cmd_group(cmd_group);
  builder_.add_h_w_dynamic(h_w_dynamic);
  builder_.add_n_dynamic(n_dynamic);
  builder_.add_is_dynamic(is_dynamic);
  builder_.add_coeff_mem(coeff_mem);
  builder_.add_output_tensor(output_tensor);
  builder_.add_input_tensor(input_tensor);
  return builder_.Finish();
}

inline flatbuffers::Offset<NetParameter> CreateNetParameterDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<flatbuffers::Offset<Tensor>> *input_tensor = nullptr,
    const std::vector<flatbuffers::Offset<Tensor>> *output_tensor = nullptr,
    uint64_t ctx_addr = 0,
    uint64_t ctx_size = 0,
    flatbuffers::Offset<CoeffMem> coeff_mem = 0,
    int32_t is_dynamic = 0,
    int32_t n_dynamic = 0,
    int32_t h_w_dynamic = 0,
    const std::vector<flatbuffers::Offset<CmdGroup>> *cmd_group = nullptr,
    const Binary *net_profile = 0,
    const std::vector<flatbuffers::Offset<StageIR>> *stage_ir = nullptr,
    const Binary *binary_ir = 0,
    const std::vector<flatbuffers::Offset<SubNet>> *sub_net = nullptr,
    uint32_t cpu_mem_size = 0,
    const std::vector<uint64_t> *ctx_sizes = nullptr,
    const Binary *net_stat = 0,
    uint32_t core_num = 0,
    uint64_t io_addr = 0,
    uint64_t io_size = 0,
    const Binary *tensor_loc = 0,
    uint64_t dynamic_ctx_addr = 0,
    uint64_t dynamic_coeff_offset = 0,
    uint64_t dynamic_combined_coeff_offset = 0) {
  auto input_tensor__ = input_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*input_tensor) : 0;
  auto output_tensor__ = output_tensor ? _fbb.CreateVector<flatbuffers::Offset<Tensor>>(*output_tensor) : 0;
  auto cmd_group__ = cmd_group ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>>(*cmd_group) : 0;
  auto stage_ir__ = stage_ir ? _fbb.CreateVector<flatbuffers::Offset<StageIR>>(*stage_ir) : 0;
  auto sub_net__ = sub_net ? _fbb.CreateVector<flatbuffers::Offset<SubNet>>(*sub_net) : 0;
  auto ctx_sizes__ = ctx_sizes ? _fbb.CreateVector<uint64_t>(*ctx_sizes) : 0;
  return bmodel::CreateNetParameter(
      _fbb,
      input_tensor__,
      output_tensor__,
      ctx_addr,
      ctx_size,
      coeff_mem,
      is_dynamic,
      n_dynamic,
      h_w_dynamic,
      cmd_group__,
      net_profile,
      stage_ir__,
      binary_ir,
      sub_net__,
      cpu_mem_size,
      ctx_sizes__,
      net_stat,
      core_num,
      io_addr,
      io_size,
      tensor_loc,
      dynamic_ctx_addr,
      dynamic_coeff_offset,
      dynamic_combined_coeff_offset);
}

flatbuffers::Offset<NetParameter> CreateNetParameter(flatbuffers::FlatBufferBuilder &_fbb, const NetParameterT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CascadeT : public flatbuffers::NativeTable {
  typedef Cascade TableType;
  uint32_t device_id;
  uint32_t step;
  std::string main_name;
  CascadeT()
      : device_id(0),
        step(0) {
  }
};

struct Cascade FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CascadeT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_ID = 4,
    VT_STEP = 6,
    VT_MAIN_NAME = 8
  };
  uint32_t device_id() const {
    return GetField<uint32_t>(VT_DEVICE_ID, 0);
  }
  bool mutate_device_id(uint32_t _device_id) {
    return SetField<uint32_t>(VT_DEVICE_ID, _device_id, 0);
  }
  uint32_t step() const {
    return GetField<uint32_t>(VT_STEP, 0);
  }
  bool mutate_step(uint32_t _step) {
    return SetField<uint32_t>(VT_STEP, _step, 0);
  }
  const flatbuffers::String *main_name() const {
    return GetPointer<const flatbuffers::String *>(VT_MAIN_NAME);
  }
  flatbuffers::String *mutable_main_name() {
    return GetPointer<flatbuffers::String *>(VT_MAIN_NAME);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_DEVICE_ID) &&
           VerifyField<uint32_t>(verifier, VT_STEP) &&
           VerifyOffset(verifier, VT_MAIN_NAME) &&
           verifier.VerifyString(main_name()) &&
           verifier.EndTable();
  }
  CascadeT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CascadeT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Cascade> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CascadeT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CascadeBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_device_id(uint32_t device_id) {
    fbb_.AddElement<uint32_t>(Cascade::VT_DEVICE_ID, device_id, 0);
  }
  void add_step(uint32_t step) {
    fbb_.AddElement<uint32_t>(Cascade::VT_STEP, step, 0);
  }
  void add_main_name(flatbuffers::Offset<flatbuffers::String> main_name) {
    fbb_.AddOffset(Cascade::VT_MAIN_NAME, main_name);
  }
  explicit CascadeBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CascadeBuilder &operator=(const CascadeBuilder &);
  flatbuffers::Offset<Cascade> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Cascade>(end);
    return o;
  }
};

inline flatbuffers::Offset<Cascade> CreateCascade(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t device_id = 0,
    uint32_t step = 0,
    flatbuffers::Offset<flatbuffers::String> main_name = 0) {
  CascadeBuilder builder_(_fbb);
  builder_.add_main_name(main_name);
  builder_.add_step(step);
  builder_.add_device_id(device_id);
  return builder_.Finish();
}

inline flatbuffers::Offset<Cascade> CreateCascadeDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t device_id = 0,
    uint32_t step = 0,
    const char *main_name = nullptr) {
  auto main_name__ = main_name ? _fbb.CreateString(main_name) : 0;
  return bmodel::CreateCascade(
      _fbb,
      device_id,
      step,
      main_name__);
}

flatbuffers::Offset<Cascade> CreateCascade(flatbuffers::FlatBufferBuilder &_fbb, const CascadeT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NetT : public flatbuffers::NativeTable {
  typedef Net TableType;
  std::string name;
  std::vector<std::unique_ptr<NetStaticT>> net_static;
  std::vector<std::unique_ptr<NetDynamicT>> net_dynamic;
  std::vector<std::unique_ptr<NetParameterT>> parameter;
  std::unique_ptr<CascadeT> cascade;
  int32_t addr_mode;
  NetT()
      : addr_mode(0) {
  }
};

struct Net FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NetT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NAME = 4,
    VT_NET_STATIC = 6,
    VT_NET_DYNAMIC = 8,
    VT_PARAMETER = 10,
    VT_CASCADE = 12,
    VT_ADDR_MODE = 14
  };
  const flatbuffers::String *name() const {
    return GetPointer<const flatbuffers::String *>(VT_NAME);
  }
  flatbuffers::String *mutable_name() {
    return GetPointer<flatbuffers::String *>(VT_NAME);
  }
  const flatbuffers::Vector<flatbuffers::Offset<NetStatic>> *net_static() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<NetStatic>> *>(VT_NET_STATIC);
  }
  flatbuffers::Vector<flatbuffers::Offset<NetStatic>> *mutable_net_static() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<NetStatic>> *>(VT_NET_STATIC);
  }
  const flatbuffers::Vector<flatbuffers::Offset<NetDynamic>> *net_dynamic() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<NetDynamic>> *>(VT_NET_DYNAMIC);
  }
  flatbuffers::Vector<flatbuffers::Offset<NetDynamic>> *mutable_net_dynamic() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<NetDynamic>> *>(VT_NET_DYNAMIC);
  }
  const flatbuffers::Vector<flatbuffers::Offset<NetParameter>> *parameter() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<NetParameter>> *>(VT_PARAMETER);
  }
  flatbuffers::Vector<flatbuffers::Offset<NetParameter>> *mutable_parameter() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<NetParameter>> *>(VT_PARAMETER);
  }
  const Cascade *cascade() const {
    return GetPointer<const Cascade *>(VT_CASCADE);
  }
  Cascade *mutable_cascade() {
    return GetPointer<Cascade *>(VT_CASCADE);
  }
  int32_t addr_mode() const {
    return GetField<int32_t>(VT_ADDR_MODE, 0);
  }
  bool mutate_addr_mode(int32_t _addr_mode) {
    return SetField<int32_t>(VT_ADDR_MODE, _addr_mode, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyOffset(verifier, VT_NET_STATIC) &&
           verifier.VerifyVector(net_static()) &&
           verifier.VerifyVectorOfTables(net_static()) &&
           VerifyOffset(verifier, VT_NET_DYNAMIC) &&
           verifier.VerifyVector(net_dynamic()) &&
           verifier.VerifyVectorOfTables(net_dynamic()) &&
           VerifyOffset(verifier, VT_PARAMETER) &&
           verifier.VerifyVector(parameter()) &&
           verifier.VerifyVectorOfTables(parameter()) &&
           VerifyOffset(verifier, VT_CASCADE) &&
           verifier.VerifyTable(cascade()) &&
           VerifyField<int32_t>(verifier, VT_ADDR_MODE) &&
           verifier.EndTable();
  }
  NetT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NetT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Net> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NetBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_name(flatbuffers::Offset<flatbuffers::String> name) {
    fbb_.AddOffset(Net::VT_NAME, name);
  }
  void add_net_static(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NetStatic>>> net_static) {
    fbb_.AddOffset(Net::VT_NET_STATIC, net_static);
  }
  void add_net_dynamic(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NetDynamic>>> net_dynamic) {
    fbb_.AddOffset(Net::VT_NET_DYNAMIC, net_dynamic);
  }
  void add_parameter(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NetParameter>>> parameter) {
    fbb_.AddOffset(Net::VT_PARAMETER, parameter);
  }
  void add_cascade(flatbuffers::Offset<Cascade> cascade) {
    fbb_.AddOffset(Net::VT_CASCADE, cascade);
  }
  void add_addr_mode(int32_t addr_mode) {
    fbb_.AddElement<int32_t>(Net::VT_ADDR_MODE, addr_mode, 0);
  }
  explicit NetBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NetBuilder &operator=(const NetBuilder &);
  flatbuffers::Offset<Net> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Net>(end);
    fbb_.Required(o, Net::VT_NAME);
    return o;
  }
};

inline flatbuffers::Offset<Net> CreateNet(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::String> name = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NetStatic>>> net_static = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NetDynamic>>> net_dynamic = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NetParameter>>> parameter = 0,
    flatbuffers::Offset<Cascade> cascade = 0,
    int32_t addr_mode = 0) {
  NetBuilder builder_(_fbb);
  builder_.add_addr_mode(addr_mode);
  builder_.add_cascade(cascade);
  builder_.add_parameter(parameter);
  builder_.add_net_dynamic(net_dynamic);
  builder_.add_net_static(net_static);
  builder_.add_name(name);
  return builder_.Finish();
}

inline flatbuffers::Offset<Net> CreateNetDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const char *name = nullptr,
    const std::vector<flatbuffers::Offset<NetStatic>> *net_static = nullptr,
    const std::vector<flatbuffers::Offset<NetDynamic>> *net_dynamic = nullptr,
    const std::vector<flatbuffers::Offset<NetParameter>> *parameter = nullptr,
    flatbuffers::Offset<Cascade> cascade = 0,
    int32_t addr_mode = 0) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  auto net_static__ = net_static ? _fbb.CreateVector<flatbuffers::Offset<NetStatic>>(*net_static) : 0;
  auto net_dynamic__ = net_dynamic ? _fbb.CreateVector<flatbuffers::Offset<NetDynamic>>(*net_dynamic) : 0;
  auto parameter__ = parameter ? _fbb.CreateVector<flatbuffers::Offset<NetParameter>>(*parameter) : 0;
  return bmodel::CreateNet(
      _fbb,
      name__,
      net_static__,
      net_dynamic__,
      parameter__,
      cascade,
      addr_mode);
}

flatbuffers::Offset<Net> CreateNet(flatbuffers::FlatBufferBuilder &_fbb, const NetT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct KernelModuleT : public flatbuffers::NativeTable {
  typedef KernelModule TableType;
  std::string file_name;
  std::unique_ptr<Binary> binary;
  KernelModuleT() {
  }
};

struct KernelModule FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef KernelModuleT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FILE_NAME = 4,
    VT_BINARY = 6
  };
  const flatbuffers::String *file_name() const {
    return GetPointer<const flatbuffers::String *>(VT_FILE_NAME);
  }
  flatbuffers::String *mutable_file_name() {
    return GetPointer<flatbuffers::String *>(VT_FILE_NAME);
  }
  const Binary *binary() const {
    return GetStruct<const Binary *>(VT_BINARY);
  }
  Binary *mutable_binary() {
    return GetStruct<Binary *>(VT_BINARY);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_FILE_NAME) &&
           verifier.VerifyString(file_name()) &&
           VerifyFieldRequired<Binary>(verifier, VT_BINARY) &&
           verifier.EndTable();
  }
  KernelModuleT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(KernelModuleT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<KernelModule> Pack(flatbuffers::FlatBufferBuilder &_fbb, const KernelModuleT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct KernelModuleBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_file_name(flatbuffers::Offset<flatbuffers::String> file_name) {
    fbb_.AddOffset(KernelModule::VT_FILE_NAME, file_name);
  }
  void add_binary(const Binary *binary) {
    fbb_.AddStruct(KernelModule::VT_BINARY, binary);
  }
  explicit KernelModuleBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  KernelModuleBuilder &operator=(const KernelModuleBuilder &);
  flatbuffers::Offset<KernelModule> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<KernelModule>(end);
    fbb_.Required(o, KernelModule::VT_FILE_NAME);
    fbb_.Required(o, KernelModule::VT_BINARY);
    return o;
  }
};

inline flatbuffers::Offset<KernelModule> CreateKernelModule(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::String> file_name = 0,
    const Binary *binary = 0) {
  KernelModuleBuilder builder_(_fbb);
  builder_.add_binary(binary);
  builder_.add_file_name(file_name);
  return builder_.Finish();
}

inline flatbuffers::Offset<KernelModule> CreateKernelModuleDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const char *file_name = nullptr,
    const Binary *binary = 0) {
  auto file_name__ = file_name ? _fbb.CreateString(file_name) : 0;
  return bmodel::CreateKernelModule(
      _fbb,
      file_name__,
      binary);
}

flatbuffers::Offset<KernelModule> CreateKernelModule(flatbuffers::FlatBufferBuilder &_fbb, const KernelModuleT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CpuopModuleT : public flatbuffers::NativeTable {
  typedef CpuopModule TableType;
  std::string file_name;
  std::unique_ptr<Binary> binary;
  CpuopModuleT() {
  }
};

struct CpuopModule FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CpuopModuleT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FILE_NAME = 4,
    VT_BINARY = 6
  };
  const flatbuffers::String *file_name() const {
    return GetPointer<const flatbuffers::String *>(VT_FILE_NAME);
  }
  flatbuffers::String *mutable_file_name() {
    return GetPointer<flatbuffers::String *>(VT_FILE_NAME);
  }
  const Binary *binary() const {
    return GetStruct<const Binary *>(VT_BINARY);
  }
  Binary *mutable_binary() {
    return GetStruct<Binary *>(VT_BINARY);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_FILE_NAME) &&
           verifier.VerifyString(file_name()) &&
           VerifyFieldRequired<Binary>(verifier, VT_BINARY) &&
           verifier.EndTable();
  }
  CpuopModuleT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CpuopModuleT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<CpuopModule> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CpuopModuleT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CpuopModuleBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_file_name(flatbuffers::Offset<flatbuffers::String> file_name) {
    fbb_.AddOffset(CpuopModule::VT_FILE_NAME, file_name);
  }
  void add_binary(const Binary *binary) {
    fbb_.AddStruct(CpuopModule::VT_BINARY, binary);
  }
  explicit CpuopModuleBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CpuopModuleBuilder &operator=(const CpuopModuleBuilder &);
  flatbuffers::Offset<CpuopModule> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<CpuopModule>(end);
    fbb_.Required(o, CpuopModule::VT_FILE_NAME);
    fbb_.Required(o, CpuopModule::VT_BINARY);
    return o;
  }
};

inline flatbuffers::Offset<CpuopModule> CreateCpuopModule(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::String> file_name = 0,
    const Binary *binary = 0) {
  CpuopModuleBuilder builder_(_fbb);
  builder_.add_binary(binary);
  builder_.add_file_name(file_name);
  return builder_.Finish();
}

inline flatbuffers::Offset<CpuopModule> CreateCpuopModuleDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const char *file_name = nullptr,
    const Binary *binary = 0) {
  auto file_name__ = file_name ? _fbb.CreateString(file_name) : 0;
  return bmodel::CreateCpuopModule(
      _fbb,
      file_name__,
      binary);
}

flatbuffers::Offset<CpuopModule> CreateCpuopModule(flatbuffers::FlatBufferBuilder &_fbb, const CpuopModuleT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ModelT : public flatbuffers::NativeTable {
  typedef Model TableType;
  std::string type;
  std::string version;
  std::string time;
  std::string chip;
  std::vector<std::unique_ptr<NetT>> net;
  uint64_t neuron_size;
  std::unique_ptr<KernelModuleT> kernel_module;
  uint32_t device_num;
  std::unique_ptr<CpuopModuleT> cpuop_module;
  uint32_t bmodel_type;
  ModelT()
      : neuron_size(0),
        device_num(0),
        bmodel_type(0) {
  }
};

struct Model FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef ModelT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TYPE = 4,
    VT_VERSION = 6,
    VT_TIME = 8,
    VT_CHIP = 10,
    VT_NET = 12,
    VT_NEURON_SIZE = 14,
    VT_KERNEL_MODULE = 16,
    VT_DEVICE_NUM = 18,
    VT_CPUOP_MODULE = 20,
    VT_BMODEL_TYPE = 22
  };
  const flatbuffers::String *type() const {
    return GetPointer<const flatbuffers::String *>(VT_TYPE);
  }
  flatbuffers::String *mutable_type() {
    return GetPointer<flatbuffers::String *>(VT_TYPE);
  }
  const flatbuffers::String *version() const {
    return GetPointer<const flatbuffers::String *>(VT_VERSION);
  }
  flatbuffers::String *mutable_version() {
    return GetPointer<flatbuffers::String *>(VT_VERSION);
  }
  const flatbuffers::String *time() const {
    return GetPointer<const flatbuffers::String *>(VT_TIME);
  }
  flatbuffers::String *mutable_time() {
    return GetPointer<flatbuffers::String *>(VT_TIME);
  }
  const flatbuffers::String *chip() const {
    return GetPointer<const flatbuffers::String *>(VT_CHIP);
  }
  flatbuffers::String *mutable_chip() {
    return GetPointer<flatbuffers::String *>(VT_CHIP);
  }
  const flatbuffers::Vector<flatbuffers::Offset<Net>> *net() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Net>> *>(VT_NET);
  }
  flatbuffers::Vector<flatbuffers::Offset<Net>> *mutable_net() {
    return GetPointer<flatbuffers::Vector<flatbuffers::Offset<Net>> *>(VT_NET);
  }
  uint64_t neuron_size() const {
    return GetField<uint64_t>(VT_NEURON_SIZE, 0);
  }
  bool mutate_neuron_size(uint64_t _neuron_size) {
    return SetField<uint64_t>(VT_NEURON_SIZE, _neuron_size, 0);
  }
  const KernelModule *kernel_module() const {
    return GetPointer<const KernelModule *>(VT_KERNEL_MODULE);
  }
  KernelModule *mutable_kernel_module() {
    return GetPointer<KernelModule *>(VT_KERNEL_MODULE);
  }
  uint32_t device_num() const {
    return GetField<uint32_t>(VT_DEVICE_NUM, 0);
  }
  bool mutate_device_num(uint32_t _device_num) {
    return SetField<uint32_t>(VT_DEVICE_NUM, _device_num, 0);
  }
  const CpuopModule *cpuop_module() const {
    return GetPointer<const CpuopModule *>(VT_CPUOP_MODULE);
  }
  CpuopModule *mutable_cpuop_module() {
    return GetPointer<CpuopModule *>(VT_CPUOP_MODULE);
  }
  uint32_t bmodel_type() const {
    return GetField<uint32_t>(VT_BMODEL_TYPE, 0);
  }
  bool mutate_bmodel_type(uint32_t _bmodel_type) {
    return SetField<uint32_t>(VT_BMODEL_TYPE, _bmodel_type, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffsetRequired(verifier, VT_TYPE) &&
           verifier.VerifyString(type()) &&
           VerifyOffsetRequired(verifier, VT_VERSION) &&
           verifier.VerifyString(version()) &&
           VerifyOffsetRequired(verifier, VT_TIME) &&
           verifier.VerifyString(time()) &&
           VerifyOffsetRequired(verifier, VT_CHIP) &&
           verifier.VerifyString(chip()) &&
           VerifyOffsetRequired(verifier, VT_NET) &&
           verifier.VerifyVector(net()) &&
           verifier.VerifyVectorOfTables(net()) &&
           VerifyField<uint64_t>(verifier, VT_NEURON_SIZE) &&
           VerifyOffset(verifier, VT_KERNEL_MODULE) &&
           verifier.VerifyTable(kernel_module()) &&
           VerifyField<uint32_t>(verifier, VT_DEVICE_NUM) &&
           VerifyOffset(verifier, VT_CPUOP_MODULE) &&
           verifier.VerifyTable(cpuop_module()) &&
           VerifyField<uint32_t>(verifier, VT_BMODEL_TYPE) &&
           verifier.EndTable();
  }
  ModelT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ModelT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Model> Pack(flatbuffers::FlatBufferBuilder &_fbb, const ModelT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ModelBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_type(flatbuffers::Offset<flatbuffers::String> type) {
    fbb_.AddOffset(Model::VT_TYPE, type);
  }
  void add_version(flatbuffers::Offset<flatbuffers::String> version) {
    fbb_.AddOffset(Model::VT_VERSION, version);
  }
  void add_time(flatbuffers::Offset<flatbuffers::String> time) {
    fbb_.AddOffset(Model::VT_TIME, time);
  }
  void add_chip(flatbuffers::Offset<flatbuffers::String> chip) {
    fbb_.AddOffset(Model::VT_CHIP, chip);
  }
  void add_net(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Net>>> net) {
    fbb_.AddOffset(Model::VT_NET, net);
  }
  void add_neuron_size(uint64_t neuron_size) {
    fbb_.AddElement<uint64_t>(Model::VT_NEURON_SIZE, neuron_size, 0);
  }
  void add_kernel_module(flatbuffers::Offset<KernelModule> kernel_module) {
    fbb_.AddOffset(Model::VT_KERNEL_MODULE, kernel_module);
  }
  void add_device_num(uint32_t device_num) {
    fbb_.AddElement<uint32_t>(Model::VT_DEVICE_NUM, device_num, 0);
  }
  void add_cpuop_module(flatbuffers::Offset<CpuopModule> cpuop_module) {
    fbb_.AddOffset(Model::VT_CPUOP_MODULE, cpuop_module);
  }
  void add_bmodel_type(uint32_t bmodel_type) {
    fbb_.AddElement<uint32_t>(Model::VT_BMODEL_TYPE, bmodel_type, 0);
  }
  explicit ModelBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ModelBuilder &operator=(const ModelBuilder &);
  flatbuffers::Offset<Model> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Model>(end);
    fbb_.Required(o, Model::VT_TYPE);
    fbb_.Required(o, Model::VT_VERSION);
    fbb_.Required(o, Model::VT_TIME);
    fbb_.Required(o, Model::VT_CHIP);
    fbb_.Required(o, Model::VT_NET);
    return o;
  }
};

inline flatbuffers::Offset<Model> CreateModel(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::String> type = 0,
    flatbuffers::Offset<flatbuffers::String> version = 0,
    flatbuffers::Offset<flatbuffers::String> time = 0,
    flatbuffers::Offset<flatbuffers::String> chip = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Net>>> net = 0,
    uint64_t neuron_size = 0,
    flatbuffers::Offset<KernelModule> kernel_module = 0,
    uint32_t device_num = 0,
    flatbuffers::Offset<CpuopModule> cpuop_module = 0,
    uint32_t bmodel_type = 0) {
  ModelBuilder builder_(_fbb);
  builder_.add_neuron_size(neuron_size);
  builder_.add_bmodel_type(bmodel_type);
  builder_.add_cpuop_module(cpuop_module);
  builder_.add_device_num(device_num);
  builder_.add_kernel_module(kernel_module);
  builder_.add_net(net);
  builder_.add_chip(chip);
  builder_.add_time(time);
  builder_.add_version(version);
  builder_.add_type(type);
  return builder_.Finish();
}

inline flatbuffers::Offset<Model> CreateModelDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const char *type = nullptr,
    const char *version = nullptr,
    const char *time = nullptr,
    const char *chip = nullptr,
    const std::vector<flatbuffers::Offset<Net>> *net = nullptr,
    uint64_t neuron_size = 0,
    flatbuffers::Offset<KernelModule> kernel_module = 0,
    uint32_t device_num = 0,
    flatbuffers::Offset<CpuopModule> cpuop_module = 0,
    uint32_t bmodel_type = 0) {
  auto type__ = type ? _fbb.CreateString(type) : 0;
  auto version__ = version ? _fbb.CreateString(version) : 0;
  auto time__ = time ? _fbb.CreateString(time) : 0;
  auto chip__ = chip ? _fbb.CreateString(chip) : 0;
  auto net__ = net ? _fbb.CreateVector<flatbuffers::Offset<Net>>(*net) : 0;
  return bmodel::CreateModel(
      _fbb,
      type__,
      version__,
      time__,
      chip__,
      net__,
      neuron_size,
      kernel_module,
      device_num,
      cpuop_module,
      bmodel_type);
}

flatbuffers::Offset<Model> CreateModel(flatbuffers::FlatBufferBuilder &_fbb, const ModelT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

inline ShapeT *Shape::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new ShapeT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Shape::UnPackTo(ShapeT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dim(); if (_e) { _o->dim.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->dim[_i] = _e->Get(_i); } } };
}

inline flatbuffers::Offset<Shape> Shape::Pack(flatbuffers::FlatBufferBuilder &_fbb, const ShapeT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateShape(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Shape> CreateShape(flatbuffers::FlatBufferBuilder &_fbb, const ShapeT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ShapeT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dim = _o->dim.size() ? _fbb.CreateVector(_o->dim) : 0;
  return bmodel::CreateShape(
      _fbb,
      _dim);
}

inline CmdGroupT *CmdGroup::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CmdGroupT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void CmdGroup::UnPackTo(CmdGroupT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = bdc_num(); _o->bdc_num = _e; };
  { auto _e = gdma_num(); _o->gdma_num = _e; };
  { auto _e = binary_bdc(); if (_e) _o->binary_bdc = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = binary_gdma(); if (_e) _o->binary_gdma = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = bdc_cmd_byte(); _o->bdc_cmd_byte = _e; };
  { auto _e = gdma_cmd_byte(); _o->gdma_cmd_byte = _e; };
}

inline flatbuffers::Offset<CmdGroup> CmdGroup::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CmdGroupT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCmdGroup(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<CmdGroup> CreateCmdGroup(flatbuffers::FlatBufferBuilder &_fbb, const CmdGroupT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CmdGroupT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _bdc_num = _o->bdc_num;
  auto _gdma_num = _o->gdma_num;
  auto _binary_bdc = _o->binary_bdc ? _o->binary_bdc.get() : 0;
  auto _binary_gdma = _o->binary_gdma ? _o->binary_gdma.get() : 0;
  auto _bdc_cmd_byte = _o->bdc_cmd_byte;
  auto _gdma_cmd_byte = _o->gdma_cmd_byte;
  return bmodel::CreateCmdGroup(
      _fbb,
      _bdc_num,
      _gdma_num,
      _binary_bdc,
      _binary_gdma,
      _bdc_cmd_byte,
      _gdma_cmd_byte);
}

inline CoreCommandsT *CoreCommands::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CoreCommandsT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void CoreCommands::UnPackTo(CoreCommandsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = gdma_tiu_commands(); if (_e) { _o->gdma_tiu_commands.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->gdma_tiu_commands[_i] = std::unique_ptr<CmdGroupT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = sdma_commands(); if (_e) { _o->sdma_commands.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sdma_commands[_i] = *_e->Get(_i); } } };
  { auto _e = hau_commands(); if (_e) { _o->hau_commands.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->hau_commands[_i] = *_e->Get(_i); } } };
  { auto _e = cdma_commands(); if (_e) { _o->cdma_commands.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->cdma_commands[_i] = *_e->Get(_i); } } };
}

inline flatbuffers::Offset<CoreCommands> CoreCommands::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CoreCommandsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCoreCommands(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<CoreCommands> CreateCoreCommands(flatbuffers::FlatBufferBuilder &_fbb, const CoreCommandsT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CoreCommandsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _gdma_tiu_commands = _o->gdma_tiu_commands.size() ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>> (_o->gdma_tiu_commands.size(), [](size_t i, _VectorArgs *__va) { return CreateCmdGroup(*__va->__fbb, __va->__o->gdma_tiu_commands[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _sdma_commands = _o->sdma_commands.size() ? _fbb.CreateVectorOfStructs(_o->sdma_commands) : 0;
  auto _hau_commands = _o->hau_commands.size() ? _fbb.CreateVectorOfStructs(_o->hau_commands) : 0;
  auto _cdma_commands = _o->cdma_commands.size() ? _fbb.CreateVectorOfStructs(_o->cdma_commands) : 0;
  return bmodel::CreateCoreCommands(
      _fbb,
      _gdma_tiu_commands,
      _sdma_commands,
      _hau_commands,
      _cdma_commands);
}

inline StageIRT *StageIR::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new StageIRT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void StageIR::UnPackTo(StageIRT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = ir_info_len(); _o->ir_info_len = _e; };
  { auto _e = height_high(); _o->height_high = _e; };
  { auto _e = height_low(); _o->height_low = _e; };
  { auto _e = width_high(); _o->width_high = _e; };
  { auto _e = width_low(); _o->width_low = _e; };
}

inline flatbuffers::Offset<StageIR> StageIR::Pack(flatbuffers::FlatBufferBuilder &_fbb, const StageIRT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateStageIR(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<StageIR> CreateStageIR(flatbuffers::FlatBufferBuilder &_fbb, const StageIRT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const StageIRT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _ir_info_len = _o->ir_info_len;
  auto _height_high = _o->height_high;
  auto _height_low = _o->height_low;
  auto _width_high = _o->width_high;
  auto _width_low = _o->width_low;
  return bmodel::CreateStageIR(
      _fbb,
      _ir_info_len,
      _height_high,
      _height_low,
      _width_high,
      _width_low);
}

inline LocationT *Location::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new LocationT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Location::UnPackTo(LocationT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = name(); if (_e) _o->name = _e->str(); };
  { auto _e = offset(); _o->offset = _e; };
  { auto _e = size(); _o->size = _e; };
}

inline flatbuffers::Offset<Location> Location::Pack(flatbuffers::FlatBufferBuilder &_fbb, const LocationT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateLocation(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Location> CreateLocation(flatbuffers::FlatBufferBuilder &_fbb, const LocationT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const LocationT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _name = _fbb.CreateString(_o->name);
  auto _offset = _o->offset;
  auto _size = _o->size;
  return bmodel::CreateLocation(
      _fbb,
      _name,
      _offset,
      _size);
}

inline CoeffMemT *CoeffMem::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CoeffMemT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void CoeffMem::UnPackTo(CoeffMemT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = address(); _o->address = _e; };
  { auto _e = check_code(); if (_e) { _o->check_code.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->check_code[_i] = _e->Get(_i); } } };
  { auto _e = binary_coeff(); if (_e) _o->binary_coeff = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = location(); if (_e) { _o->location.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->location[_i] = std::unique_ptr<LocationT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = encrypt_mode(); _o->encrypt_mode = _e; };
  { auto _e = decrypt_size(); _o->decrypt_size = _e; };
}

inline flatbuffers::Offset<CoeffMem> CoeffMem::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CoeffMemT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCoeffMem(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<CoeffMem> CreateCoeffMem(flatbuffers::FlatBufferBuilder &_fbb, const CoeffMemT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CoeffMemT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _address = _o->address;
  auto _check_code = _o->check_code.size() ? _fbb.CreateVector(_o->check_code) : 0;
  auto _binary_coeff = _o->binary_coeff ? _o->binary_coeff.get() : 0;
  auto _location = _o->location.size() ? _fbb.CreateVector<flatbuffers::Offset<Location>> (_o->location.size(), [](size_t i, _VectorArgs *__va) { return CreateLocation(*__va->__fbb, __va->__o->location[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _encrypt_mode = _o->encrypt_mode;
  auto _decrypt_size = _o->decrypt_size;
  return bmodel::CreateCoeffMem(
      _fbb,
      _address,
      _check_code,
      _binary_coeff,
      _location,
      _encrypt_mode,
      _decrypt_size);
}

inline TensorT *Tensor::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new TensorT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Tensor::UnPackTo(TensorT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = name(); if (_e) _o->name = _e->str(); };
  { auto _e = data_type(); _o->data_type = _e; };
  { auto _e = gmem_stmode(); _o->gmem_stmode = _e; };
  { auto _e = device_addr(); _o->device_addr = _e; };
  { auto _e = size(); _o->size = _e; };
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = std::unique_ptr<ShapeT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = mem_type(); _o->mem_type = _e; };
  { auto _e = scale(); _o->scale = _e; };
  { auto _e = cpu_addr(); _o->cpu_addr = _e; };
  { auto _e = pad_h(); _o->pad_h = _e; };
  { auto _e = zero_point(); _o->zero_point = _e; };
  { auto _e = hidden(); _o->hidden = _e; };
  { auto _e = index(); _o->index = _e; };
}

inline flatbuffers::Offset<Tensor> Tensor::Pack(flatbuffers::FlatBufferBuilder &_fbb, const TensorT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensor(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Tensor> CreateTensor(flatbuffers::FlatBufferBuilder &_fbb, const TensorT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const TensorT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _name = _fbb.CreateString(_o->name);
  auto _data_type = _o->data_type;
  auto _gmem_stmode = _o->gmem_stmode;
  auto _device_addr = _o->device_addr;
  auto _size = _o->size;
  auto _shape = _o->shape.size() ? _fbb.CreateVector<flatbuffers::Offset<Shape>> (_o->shape.size(), [](size_t i, _VectorArgs *__va) { return CreateShape(*__va->__fbb, __va->__o->shape[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _mem_type = _o->mem_type;
  auto _scale = _o->scale;
  auto _cpu_addr = _o->cpu_addr;
  auto _pad_h = _o->pad_h;
  auto _zero_point = _o->zero_point;
  auto _hidden = _o->hidden;
  auto _index = _o->index;
  return bmodel::CreateTensor(
      _fbb,
      _name,
      _data_type,
      _gmem_stmode,
      _device_addr,
      _size,
      _shape,
      _mem_type,
      _scale,
      _cpu_addr,
      _pad_h,
      _zero_point,
      _hidden,
      _index);
}

inline CpuConstT *CpuConst::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CpuConstT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void CpuConst::UnPackTo(CpuConstT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = name(); if (_e) _o->name = _e->str(); };
  { auto _e = const_data(); if (_e) _o->const_data = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = check_code(); if (_e) { _o->check_code.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->check_code[_i] = _e->Get(_i); } } };
}

inline flatbuffers::Offset<CpuConst> CpuConst::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CpuConstT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCpuConst(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<CpuConst> CreateCpuConst(flatbuffers::FlatBufferBuilder &_fbb, const CpuConstT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CpuConstT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
  auto _const_data = _o->const_data ? _o->const_data.get() : 0;
  auto _check_code = _o->check_code.size() ? _fbb.CreateVector(_o->check_code) : 0;
  return bmodel::CreateCpuConst(
      _fbb,
      _name,
      _const_data,
      _check_code);
}

inline CpuParamT *CpuParam::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CpuParamT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void CpuParam::UnPackTo(CpuParamT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = op_type(); _o->op_type = _e; };
  { auto _e = binary_param(); if (_e) _o->binary_param = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = cpu_const(); if (_e) { _o->cpu_const.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->cpu_const[_i] = std::unique_ptr<CpuConstT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = is_custom(); _o->is_custom = _e; };
}

inline flatbuffers::Offset<CpuParam> CpuParam::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CpuParamT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCpuParam(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<CpuParam> CreateCpuParam(flatbuffers::FlatBufferBuilder &_fbb, const CpuParamT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CpuParamT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _op_type = _o->op_type;
  auto _binary_param = _o->binary_param ? _o->binary_param.get() : 0;
  auto _cpu_const = _o->cpu_const.size() ? _fbb.CreateVector<flatbuffers::Offset<CpuConst>> (_o->cpu_const.size(), [](size_t i, _VectorArgs *__va) { return CreateCpuConst(*__va->__fbb, __va->__o->cpu_const[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _is_custom = _o->is_custom;
  return bmodel::CreateCpuParam(
      _fbb,
      _op_type,
      _binary_param,
      _cpu_const,
      _is_custom);
}

inline OutputFromT *OutputFrom::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new OutputFromT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void OutputFrom::UnPackTo(OutputFromT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = indice(); if (_e) { _o->indice.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->indice[_i] = _e->Get(_i); } } };
}

inline flatbuffers::Offset<OutputFrom> OutputFrom::Pack(flatbuffers::FlatBufferBuilder &_fbb, const OutputFromT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateOutputFrom(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<OutputFrom> CreateOutputFrom(flatbuffers::FlatBufferBuilder &_fbb, const OutputFromT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const OutputFromT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _indice = _o->indice.size() ? _fbb.CreateVector(_o->indice) : 0;
  return bmodel::CreateOutputFrom(
      _fbb,
      _indice);
}

inline MergeParamT *MergeParam::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new MergeParamT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void MergeParam::UnPackTo(MergeParamT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = output_from(); if (_e) { _o->output_from.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_from[_i] = std::unique_ptr<OutputFromT>(_e->Get(_i)->UnPack(_resolver)); } } };
}

inline flatbuffers::Offset<MergeParam> MergeParam::Pack(flatbuffers::FlatBufferBuilder &_fbb, const MergeParamT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateMergeParam(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<MergeParam> CreateMergeParam(flatbuffers::FlatBufferBuilder &_fbb, const MergeParamT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const MergeParamT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _output_from = _o->output_from.size() ? _fbb.CreateVector<flatbuffers::Offset<OutputFrom>> (_o->output_from.size(), [](size_t i, _VectorArgs *__va) { return CreateOutputFrom(*__va->__fbb, __va->__o->output_from[i].get(), __va->__rehasher); }, &_va ) : 0;
  return bmodel::CreateMergeParam(
      _fbb,
      _output_from);
}

inline SwitchParamT *SwitchParam::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new SwitchParamT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void SwitchParam::UnPackTo(SwitchParamT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = output_from(); if (_e) { _o->output_from.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_from[_i] = _e->Get(_i); } } };
  { auto _e = output_branch(); if (_e) { _o->output_branch.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_branch[_i] = _e->Get(_i); } } };
}

inline flatbuffers::Offset<SwitchParam> SwitchParam::Pack(flatbuffers::FlatBufferBuilder &_fbb, const SwitchParamT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSwitchParam(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<SwitchParam> CreateSwitchParam(flatbuffers::FlatBufferBuilder &_fbb, const SwitchParamT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SwitchParamT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _output_from = _o->output_from.size() ? _fbb.CreateVector(_o->output_from) : 0;
  auto _output_branch = _o->output_branch.size() ? _fbb.CreateVector(_o->output_branch) : 0;
  return bmodel::CreateSwitchParam(
      _fbb,
      _output_from,
      _output_branch);
}

inline SubNetT *SubNet::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new SubNetT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void SubNet::UnPackTo(SubNetT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = subnet_mode(); _o->subnet_mode = _e; };
  { auto _e = cmd_group(); if (_e) { _o->cmd_group.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->cmd_group[_i] = std::unique_ptr<CmdGroupT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = cpu_param(); if (_e) { _o->cpu_param.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->cpu_param[_i] = std::unique_ptr<CpuParamT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = input_tensor(); if (_e) { _o->input_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->input_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = output_tensor(); if (_e) { _o->output_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = is_dynamic(); _o->is_dynamic = _e; };
  { auto _e = ir_offset(); _o->ir_offset = _e; };
  { auto _e = ir_len(); _o->ir_len = _e; };
  { auto _e = n_dynamic(); _o->n_dynamic = _e; };
  { auto _e = h_w_dynamic(); _o->h_w_dynamic = _e; };
  { auto _e = id(); _o->id = _e; };
  { auto _e = next_subnet_ids(); if (_e) { _o->next_subnet_ids.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->next_subnet_ids[_i] = _e->Get(_i); } } };
  { auto _e = merge_param(); if (_e) _o->merge_param = std::unique_ptr<MergeParamT>(_e->UnPack(_resolver)); };
  { auto _e = switch_param(); if (_e) _o->switch_param = std::unique_ptr<SwitchParamT>(_e->UnPack(_resolver)); };
  { auto _e = core_commands(); if (_e) { _o->core_commands.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->core_commands[_i] = std::unique_ptr<CoreCommandsT>(_e->Get(_i)->UnPack(_resolver)); } } };
}

inline flatbuffers::Offset<SubNet> SubNet::Pack(flatbuffers::FlatBufferBuilder &_fbb, const SubNetT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSubNet(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<SubNet> CreateSubNet(flatbuffers::FlatBufferBuilder &_fbb, const SubNetT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const SubNetT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _subnet_mode = _o->subnet_mode;
  auto _cmd_group = _o->cmd_group.size() ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>> (_o->cmd_group.size(), [](size_t i, _VectorArgs *__va) { return CreateCmdGroup(*__va->__fbb, __va->__o->cmd_group[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _cpu_param = _o->cpu_param.size() ? _fbb.CreateVector<flatbuffers::Offset<CpuParam>> (_o->cpu_param.size(), [](size_t i, _VectorArgs *__va) { return CreateCpuParam(*__va->__fbb, __va->__o->cpu_param[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _input_tensor = _o->input_tensor.size() ? _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->input_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->input_tensor[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _output_tensor = _o->output_tensor.size() ? _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->output_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->output_tensor[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _is_dynamic = _o->is_dynamic;
  auto _ir_offset = _o->ir_offset;
  auto _ir_len = _o->ir_len;
  auto _n_dynamic = _o->n_dynamic;
  auto _h_w_dynamic = _o->h_w_dynamic;
  auto _id = _o->id;
  auto _next_subnet_ids = _o->next_subnet_ids.size() ? _fbb.CreateVector(_o->next_subnet_ids) : 0;
  auto _merge_param = _o->merge_param ? CreateMergeParam(_fbb, _o->merge_param.get(), _rehasher) : 0;
  auto _switch_param = _o->switch_param ? CreateSwitchParam(_fbb, _o->switch_param.get(), _rehasher) : 0;
  auto _core_commands = _o->core_commands.size() ? _fbb.CreateVector<flatbuffers::Offset<CoreCommands>> (_o->core_commands.size(), [](size_t i, _VectorArgs *__va) { return CreateCoreCommands(*__va->__fbb, __va->__o->core_commands[i].get(), __va->__rehasher); }, &_va ) : 0;
  return bmodel::CreateSubNet(
      _fbb,
      _subnet_mode,
      _cmd_group,
      _cpu_param,
      _input_tensor,
      _output_tensor,
      _is_dynamic,
      _ir_offset,
      _ir_len,
      _n_dynamic,
      _h_w_dynamic,
      _id,
      _next_subnet_ids,
      _merge_param,
      _switch_param,
      _core_commands);
}

inline NetStaticT *NetStatic::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NetStaticT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void NetStatic::UnPackTo(NetStaticT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = input_tensor(); if (_e) { _o->input_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->input_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = output_tensor(); if (_e) { _o->output_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = cmd_group(); if (_e) { _o->cmd_group.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->cmd_group[_i] = std::unique_ptr<CmdGroupT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = ctx_addr(); _o->ctx_addr = _e; };
  { auto _e = ctx_size(); _o->ctx_size = _e; };
  { auto _e = coeff_mem(); if (_e) _o->coeff_mem = std::unique_ptr<CoeffMemT>(_e->UnPack(_resolver)); };
  { auto _e = sub_net(); if (_e) { _o->sub_net.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sub_net[_i] = std::unique_ptr<SubNetT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = net_profile(); if (_e) _o->net_profile = std::unique_ptr<Binary>(new Binary(*_e)); };
}

inline flatbuffers::Offset<NetStatic> NetStatic::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetStaticT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNetStatic(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<NetStatic> CreateNetStatic(flatbuffers::FlatBufferBuilder &_fbb, const NetStaticT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NetStaticT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _input_tensor = _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->input_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->input_tensor[i].get(), __va->__rehasher); }, &_va );
  auto _output_tensor = _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->output_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->output_tensor[i].get(), __va->__rehasher); }, &_va );
  auto _cmd_group = _o->cmd_group.size() ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>> (_o->cmd_group.size(), [](size_t i, _VectorArgs *__va) { return CreateCmdGroup(*__va->__fbb, __va->__o->cmd_group[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _ctx_addr = _o->ctx_addr;
  auto _ctx_size = _o->ctx_size;
  auto _coeff_mem = _o->coeff_mem ? CreateCoeffMem(_fbb, _o->coeff_mem.get(), _rehasher) : 0;
  auto _sub_net = _o->sub_net.size() ? _fbb.CreateVector<flatbuffers::Offset<SubNet>> (_o->sub_net.size(), [](size_t i, _VectorArgs *__va) { return CreateSubNet(*__va->__fbb, __va->__o->sub_net[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _net_profile = _o->net_profile ? _o->net_profile.get() : 0;
  return bmodel::CreateNetStatic(
      _fbb,
      _input_tensor,
      _output_tensor,
      _cmd_group,
      _ctx_addr,
      _ctx_size,
      _coeff_mem,
      _sub_net,
      _net_profile);
}

inline NetDynamicT *NetDynamic::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NetDynamicT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void NetDynamic::UnPackTo(NetDynamicT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = input_tensor(); if (_e) { _o->input_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->input_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = output_tensor(); if (_e) { _o->output_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = n_dynamic(); _o->n_dynamic = _e; };
  { auto _e = h_w_dynamic(); _o->h_w_dynamic = _e; };
  { auto _e = stage_ir(); if (_e) { _o->stage_ir.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->stage_ir[_i] = std::unique_ptr<StageIRT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = binary_ir(); if (_e) _o->binary_ir = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = ctx_addr(); _o->ctx_addr = _e; };
  { auto _e = ctx_size(); _o->ctx_size = _e; };
  { auto _e = coeff_mem(); if (_e) _o->coeff_mem = std::unique_ptr<CoeffMemT>(_e->UnPack(_resolver)); };
  { auto _e = sub_net(); if (_e) { _o->sub_net.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sub_net[_i] = std::unique_ptr<SubNetT>(_e->Get(_i)->UnPack(_resolver)); } } };
}

inline flatbuffers::Offset<NetDynamic> NetDynamic::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetDynamicT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNetDynamic(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<NetDynamic> CreateNetDynamic(flatbuffers::FlatBufferBuilder &_fbb, const NetDynamicT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NetDynamicT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _input_tensor = _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->input_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->input_tensor[i].get(), __va->__rehasher); }, &_va );
  auto _output_tensor = _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->output_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->output_tensor[i].get(), __va->__rehasher); }, &_va );
  auto _n_dynamic = _o->n_dynamic;
  auto _h_w_dynamic = _o->h_w_dynamic;
  auto _stage_ir = _o->stage_ir.size() ? _fbb.CreateVector<flatbuffers::Offset<StageIR>> (_o->stage_ir.size(), [](size_t i, _VectorArgs *__va) { return CreateStageIR(*__va->__fbb, __va->__o->stage_ir[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _binary_ir = _o->binary_ir ? _o->binary_ir.get() : 0;
  auto _ctx_addr = _o->ctx_addr;
  auto _ctx_size = _o->ctx_size;
  auto _coeff_mem = _o->coeff_mem ? CreateCoeffMem(_fbb, _o->coeff_mem.get(), _rehasher) : 0;
  auto _sub_net = _o->sub_net.size() ? _fbb.CreateVector<flatbuffers::Offset<SubNet>> (_o->sub_net.size(), [](size_t i, _VectorArgs *__va) { return CreateSubNet(*__va->__fbb, __va->__o->sub_net[i].get(), __va->__rehasher); }, &_va ) : 0;
  return bmodel::CreateNetDynamic(
      _fbb,
      _input_tensor,
      _output_tensor,
      _n_dynamic,
      _h_w_dynamic,
      _stage_ir,
      _binary_ir,
      _ctx_addr,
      _ctx_size,
      _coeff_mem,
      _sub_net);
}

inline NetParameterT *NetParameter::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NetParameterT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void NetParameter::UnPackTo(NetParameterT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = input_tensor(); if (_e) { _o->input_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->input_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = output_tensor(); if (_e) { _o->output_tensor.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_tensor[_i] = std::unique_ptr<TensorT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = ctx_addr(); _o->ctx_addr = _e; };
  { auto _e = ctx_size(); _o->ctx_size = _e; };
  { auto _e = coeff_mem(); if (_e) _o->coeff_mem = std::unique_ptr<CoeffMemT>(_e->UnPack(_resolver)); };
  { auto _e = is_dynamic(); _o->is_dynamic = _e; };
  { auto _e = n_dynamic(); _o->n_dynamic = _e; };
  { auto _e = h_w_dynamic(); _o->h_w_dynamic = _e; };
  { auto _e = cmd_group(); if (_e) { _o->cmd_group.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->cmd_group[_i] = std::unique_ptr<CmdGroupT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = net_profile(); if (_e) _o->net_profile = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = stage_ir(); if (_e) { _o->stage_ir.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->stage_ir[_i] = std::unique_ptr<StageIRT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = binary_ir(); if (_e) _o->binary_ir = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = sub_net(); if (_e) { _o->sub_net.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sub_net[_i] = std::unique_ptr<SubNetT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = cpu_mem_size(); _o->cpu_mem_size = _e; };
  { auto _e = ctx_sizes(); if (_e) { _o->ctx_sizes.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->ctx_sizes[_i] = _e->Get(_i); } } };
  { auto _e = net_stat(); if (_e) _o->net_stat = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = core_num(); _o->core_num = _e; };
  { auto _e = io_addr(); _o->io_addr = _e; };
  { auto _e = io_size(); _o->io_size = _e; };
  { auto _e = tensor_loc(); if (_e) _o->tensor_loc = std::unique_ptr<Binary>(new Binary(*_e)); };
  { auto _e = dynamic_ctx_addr(); _o->dynamic_ctx_addr = _e; };
  { auto _e = dynamic_coeff_offset(); _o->dynamic_coeff_offset = _e; };
  { auto _e = dynamic_combined_coeff_offset(); _o->dynamic_combined_coeff_offset = _e; };
}

inline flatbuffers::Offset<NetParameter> NetParameter::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetParameterT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNetParameter(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<NetParameter> CreateNetParameter(flatbuffers::FlatBufferBuilder &_fbb, const NetParameterT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NetParameterT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _input_tensor = _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->input_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->input_tensor[i].get(), __va->__rehasher); }, &_va );
  auto _output_tensor = _fbb.CreateVector<flatbuffers::Offset<Tensor>> (_o->output_tensor.size(), [](size_t i, _VectorArgs *__va) { return CreateTensor(*__va->__fbb, __va->__o->output_tensor[i].get(), __va->__rehasher); }, &_va );
  auto _ctx_addr = _o->ctx_addr;
  auto _ctx_size = _o->ctx_size;
  auto _coeff_mem = _o->coeff_mem ? CreateCoeffMem(_fbb, _o->coeff_mem.get(), _rehasher) : 0;
  auto _is_dynamic = _o->is_dynamic;
  auto _n_dynamic = _o->n_dynamic;
  auto _h_w_dynamic = _o->h_w_dynamic;
  auto _cmd_group = _o->cmd_group.size() ? _fbb.CreateVector<flatbuffers::Offset<CmdGroup>> (_o->cmd_group.size(), [](size_t i, _VectorArgs *__va) { return CreateCmdGroup(*__va->__fbb, __va->__o->cmd_group[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _net_profile = _o->net_profile ? _o->net_profile.get() : 0;
  auto _stage_ir = _o->stage_ir.size() ? _fbb.CreateVector<flatbuffers::Offset<StageIR>> (_o->stage_ir.size(), [](size_t i, _VectorArgs *__va) { return CreateStageIR(*__va->__fbb, __va->__o->stage_ir[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _binary_ir = _o->binary_ir ? _o->binary_ir.get() : 0;
  auto _sub_net = _o->sub_net.size() ? _fbb.CreateVector<flatbuffers::Offset<SubNet>> (_o->sub_net.size(), [](size_t i, _VectorArgs *__va) { return CreateSubNet(*__va->__fbb, __va->__o->sub_net[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _cpu_mem_size = _o->cpu_mem_size;
  auto _ctx_sizes = _o->ctx_sizes.size() ? _fbb.CreateVector(_o->ctx_sizes) : 0;
  auto _net_stat = _o->net_stat ? _o->net_stat.get() : 0;
  auto _core_num = _o->core_num;
  auto _io_addr = _o->io_addr;
  auto _io_size = _o->io_size;
  auto _tensor_loc = _o->tensor_loc ? _o->tensor_loc.get() : 0;
  auto _dynamic_ctx_addr = _o->dynamic_ctx_addr;
  auto _dynamic_coeff_offset = _o->dynamic_coeff_offset;
  auto _dynamic_combined_coeff_offset = _o->dynamic_combined_coeff_offset;
  return bmodel::CreateNetParameter(
      _fbb,
      _input_tensor,
      _output_tensor,
      _ctx_addr,
      _ctx_size,
      _coeff_mem,
      _is_dynamic,
      _n_dynamic,
      _h_w_dynamic,
      _cmd_group,
      _net_profile,
      _stage_ir,
      _binary_ir,
      _sub_net,
      _cpu_mem_size,
      _ctx_sizes,
      _net_stat,
      _core_num,
      _io_addr,
      _io_size,
      _tensor_loc,
      _dynamic_ctx_addr,
      _dynamic_coeff_offset,
      _dynamic_combined_coeff_offset);
}

inline CascadeT *Cascade::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CascadeT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Cascade::UnPackTo(CascadeT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = device_id(); _o->device_id = _e; };
  { auto _e = step(); _o->step = _e; };
  { auto _e = main_name(); if (_e) _o->main_name = _e->str(); };
}

inline flatbuffers::Offset<Cascade> Cascade::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CascadeT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCascade(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Cascade> CreateCascade(flatbuffers::FlatBufferBuilder &_fbb, const CascadeT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CascadeT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _device_id = _o->device_id;
  auto _step = _o->step;
  auto _main_name = _o->main_name.empty() ? 0 : _fbb.CreateString(_o->main_name);
  return bmodel::CreateCascade(
      _fbb,
      _device_id,
      _step,
      _main_name);
}

inline NetT *Net::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NetT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Net::UnPackTo(NetT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = name(); if (_e) _o->name = _e->str(); };
  { auto _e = net_static(); if (_e) { _o->net_static.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->net_static[_i] = std::unique_ptr<NetStaticT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = net_dynamic(); if (_e) { _o->net_dynamic.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->net_dynamic[_i] = std::unique_ptr<NetDynamicT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = parameter(); if (_e) { _o->parameter.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->parameter[_i] = std::unique_ptr<NetParameterT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = cascade(); if (_e) _o->cascade = std::unique_ptr<CascadeT>(_e->UnPack(_resolver)); };
  { auto _e = addr_mode(); _o->addr_mode = _e; };
}

inline flatbuffers::Offset<Net> Net::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NetT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNet(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Net> CreateNet(flatbuffers::FlatBufferBuilder &_fbb, const NetT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NetT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _name = _fbb.CreateString(_o->name);
  auto _net_static = _o->net_static.size() ? _fbb.CreateVector<flatbuffers::Offset<NetStatic>> (_o->net_static.size(), [](size_t i, _VectorArgs *__va) { return CreateNetStatic(*__va->__fbb, __va->__o->net_static[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _net_dynamic = _o->net_dynamic.size() ? _fbb.CreateVector<flatbuffers::Offset<NetDynamic>> (_o->net_dynamic.size(), [](size_t i, _VectorArgs *__va) { return CreateNetDynamic(*__va->__fbb, __va->__o->net_dynamic[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _parameter = _o->parameter.size() ? _fbb.CreateVector<flatbuffers::Offset<NetParameter>> (_o->parameter.size(), [](size_t i, _VectorArgs *__va) { return CreateNetParameter(*__va->__fbb, __va->__o->parameter[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _cascade = _o->cascade ? CreateCascade(_fbb, _o->cascade.get(), _rehasher) : 0;
  auto _addr_mode = _o->addr_mode;
  return bmodel::CreateNet(
      _fbb,
      _name,
      _net_static,
      _net_dynamic,
      _parameter,
      _cascade,
      _addr_mode);
}

inline KernelModuleT *KernelModule::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new KernelModuleT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void KernelModule::UnPackTo(KernelModuleT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = file_name(); if (_e) _o->file_name = _e->str(); };
  { auto _e = binary(); if (_e) _o->binary = std::unique_ptr<Binary>(new Binary(*_e)); };
}

inline flatbuffers::Offset<KernelModule> KernelModule::Pack(flatbuffers::FlatBufferBuilder &_fbb, const KernelModuleT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateKernelModule(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<KernelModule> CreateKernelModule(flatbuffers::FlatBufferBuilder &_fbb, const KernelModuleT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const KernelModuleT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _file_name = _fbb.CreateString(_o->file_name);
  auto _binary = _o->binary ? _o->binary.get() : 0;
  return bmodel::CreateKernelModule(
      _fbb,
      _file_name,
      _binary);
}

inline CpuopModuleT *CpuopModule::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CpuopModuleT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void CpuopModule::UnPackTo(CpuopModuleT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = file_name(); if (_e) _o->file_name = _e->str(); };
  { auto _e = binary(); if (_e) _o->binary = std::unique_ptr<Binary>(new Binary(*_e)); };
}

inline flatbuffers::Offset<CpuopModule> CpuopModule::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CpuopModuleT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCpuopModule(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<CpuopModule> CreateCpuopModule(flatbuffers::FlatBufferBuilder &_fbb, const CpuopModuleT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CpuopModuleT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _file_name = _fbb.CreateString(_o->file_name);
  auto _binary = _o->binary ? _o->binary.get() : 0;
  return bmodel::CreateCpuopModule(
      _fbb,
      _file_name,
      _binary);
}

inline ModelT *Model::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new ModelT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Model::UnPackTo(ModelT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = type(); if (_e) _o->type = _e->str(); };
  { auto _e = version(); if (_e) _o->version = _e->str(); };
  { auto _e = time(); if (_e) _o->time = _e->str(); };
  { auto _e = chip(); if (_e) _o->chip = _e->str(); };
  { auto _e = net(); if (_e) { _o->net.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->net[_i] = std::unique_ptr<NetT>(_e->Get(_i)->UnPack(_resolver)); } } };
  { auto _e = neuron_size(); _o->neuron_size = _e; };
  { auto _e = kernel_module(); if (_e) _o->kernel_module = std::unique_ptr<KernelModuleT>(_e->UnPack(_resolver)); };
  { auto _e = device_num(); _o->device_num = _e; };
  { auto _e = cpuop_module(); if (_e) _o->cpuop_module = std::unique_ptr<CpuopModuleT>(_e->UnPack(_resolver)); };
  { auto _e = bmodel_type(); _o->bmodel_type = _e; };
}

inline flatbuffers::Offset<Model> Model::Pack(flatbuffers::FlatBufferBuilder &_fbb, const ModelT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateModel(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Model> CreateModel(flatbuffers::FlatBufferBuilder &_fbb, const ModelT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const ModelT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _type = _fbb.CreateString(_o->type);
  auto _version = _fbb.CreateString(_o->version);
  auto _time = _fbb.CreateString(_o->time);
  auto _chip = _fbb.CreateString(_o->chip);
  auto _net = _fbb.CreateVector<flatbuffers::Offset<Net>> (_o->net.size(), [](size_t i, _VectorArgs *__va) { return CreateNet(*__va->__fbb, __va->__o->net[i].get(), __va->__rehasher); }, &_va );
  auto _neuron_size = _o->neuron_size;
  auto _kernel_module = _o->kernel_module ? CreateKernelModule(_fbb, _o->kernel_module.get(), _rehasher) : 0;
  auto _device_num = _o->device_num;
  auto _cpuop_module = _o->cpuop_module ? CreateCpuopModule(_fbb, _o->cpuop_module.get(), _rehasher) : 0;
  auto _bmodel_type = _o->bmodel_type;
  return bmodel::CreateModel(
      _fbb,
      _type,
      _version,
      _time,
      _chip,
      _net,
      _neuron_size,
      _kernel_module,
      _device_num,
      _cpuop_module,
      _bmodel_type);
}

inline const bmodel::Model *GetModel(const void *buf) {
  return flatbuffers::GetRoot<bmodel::Model>(buf);
}

inline const bmodel::Model *GetSizePrefixedModel(const void *buf) {
  return flatbuffers::GetSizePrefixedRoot<bmodel::Model>(buf);
}

inline Model *GetMutableModel(void *buf) {
  return flatbuffers::GetMutableRoot<Model>(buf);
}

inline bool VerifyModelBuffer(
    flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<bmodel::Model>(nullptr);
}

inline bool VerifySizePrefixedModelBuffer(
    flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<bmodel::Model>(nullptr);
}

inline void FinishModelBuffer(
    flatbuffers::FlatBufferBuilder &fbb,
    flatbuffers::Offset<bmodel::Model> root) {
  fbb.Finish(root);
}

inline void FinishSizePrefixedModelBuffer(
    flatbuffers::FlatBufferBuilder &fbb,
    flatbuffers::Offset<bmodel::Model> root) {
  fbb.FinishSizePrefixed(root);
}

inline std::unique_ptr<ModelT> UnPackModel(
    const void *buf,
    const flatbuffers::resolver_function_t *res = nullptr) {
  return std::unique_ptr<ModelT>(GetModel(buf)->UnPack(res));
}

}  // namespace bmodel

#endif  // FLATBUFFERS_GENERATED_MODEL_BMODEL_H_
